{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter10. 케라스를 사용한 인공 신경망 소개",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpiy9pM7UI_A"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTxCrauoYExE"
      },
      "source": [
        "#10.1 생물학적 뉴런에서 인공 뉴런까지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaNm8dCzYEmy"
      },
      "source": [
        "##10.1.1 생물학적 뉴런"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTU9Gjg1YEdY"
      },
      "source": [
        "##10.1.2 뉴런을 사용한 논리 연산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lbIgr_hYETX"
      },
      "source": [
        "##10.1.3 퍼셉트론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_SHNu7aYEJn"
      },
      "source": [
        "##10.1.4 다층 퍼셉트론과 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYcv48W0YEAc"
      },
      "source": [
        "##10.1.5 회귀를 위한 다층 퍼셉트론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIn6K0umYD22"
      },
      "source": [
        "##10.1.6 분류를 위한 다층 퍼셉트론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyIwdJRhYDtk"
      },
      "source": [
        "#10.2 케라스로 다층 퍼셉트론 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVHqZuusYDjT"
      },
      "source": [
        "##10.2.1 텐서플로 2 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WicGfq0pPYm9",
        "outputId": "bfe89fbd-0389-4107-bb2f-38d57e24f81d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cGvv3r-0PrUU",
        "outputId": "6ab84282-77b2-458a-e8e3-4ab14a028ca9"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NP4hZQbYDZg"
      },
      "source": [
        "##10.2.2 시퀀셜 API를 사용하여 이미지 분류기 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9L-ucI0YDRB"
      },
      "source": [
        "###케라스를 사용하여 데이터셋 적재하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9D70lfDPvrT",
        "outputId": "290d7dcb-1477-4686-86dc-714ab4cd73c3"
      },
      "source": [
        "#데이터 불러오기//\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exj_p-XSP7j1",
        "outputId": "5b4df760-ab01-4d42-bdea-a1c4c97d2d65"
      },
      "source": [
        "print(X_train_full.shape)\n",
        "\n",
        "#데이터 타입이 0~255의 정수형..\n",
        "print(X_train_full.dtype)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFHtQxy8QHYJ"
      },
      "source": [
        "#검증세트 분리하고 스케일링까지..\n",
        "\n",
        "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOpHVCirQgXT"
      },
      "source": [
        "#클래스 이름 준비..\n",
        "\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UBAc1k5OQsfX",
        "outputId": "6705213d-bf42-4bcf-e539-fe7b309889fd"
      },
      "source": [
        "class_names[y_train[0]]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Coat'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5zbybtUYDIN"
      },
      "source": [
        "###시퀀셜 API를 사용하여 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWZe6dP4RguU"
      },
      "source": [
        "층을 하나씩 추가하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24sY_apuRCAw"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Flatten(input_shape=[28,28]))        #입력모양 명시..\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))       #1번째 은닉층\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))       #2번째 은닉층\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))     #다중분류(10개의 클래스)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf-OOE8yRjiv"
      },
      "source": [
        "Sequential 모델을 만들때 층의 리스트를 전달하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcrJpMgSRnyp"
      },
      "source": [
        "#위의 층과 순서 같음..\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28,28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCn7imUySOsn"
      },
      "source": [
        "모델의 요약본을 보여주는 summary()메서드\n",
        "\n",
        "- Dense층은 파라미터가 많다.. -> **유연성**을 가지지만 **과대적합 위험**도 있다.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fS8aynVSHQ_",
        "outputId": "93412ca2-5b45-499d-a77f-e5e8207d9d82"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGriYqi5SinL",
        "outputId": "a11a6fe1-c092-4ad1-d2bb-ef64efa781b0"
      },
      "source": [
        "#모든 층 확인..\n",
        "model.layers"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.core.Flatten at 0x7f2135b9e690>,\n",
              " <keras.layers.core.Dense at 0x7f2131922890>,\n",
              " <keras.layers.core.Dense at 0x7f2131922650>,\n",
              " <keras.layers.core.Dense at 0x7f2131922cd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0PCP9ErQSnHr",
        "outputId": "81aabcf5-3449-4190-ce8c-c46f377cd240"
      },
      "source": [
        "#층을 인덱싱으로 가져올 수도 있다..\n",
        "hidden1 = model.layers[1]\n",
        "hidden1.name"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dense_6'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWJPSUiXSuFD",
        "outputId": "9f780078-3d3f-4f5a-f307-18f64b2bf767"
      },
      "source": [
        "#층을 이름으로 가져오기..\n",
        "model.get_layer('dense_6') is hidden1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjDf0AkaTCJM",
        "outputId": "e4f74f18-0290-46b9-882f-7405d37b4f57"
      },
      "source": [
        "#파라미터는 get_weights()와 set_weights()를 사용해서 접근 가능\n",
        "\n",
        "weights, biases = hidden1.get_weights()\n",
        "\n",
        "print(weights)\n",
        "print(weights.shape)\n",
        "print(biases)\n",
        "print(biases.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.05272421  0.0574936   0.02540258 ... -0.06202113 -0.07272809\n",
            "  -0.0493255 ]\n",
            " [-0.00996617  0.05068396  0.0721685  ...  0.06006375 -0.01628917\n",
            "   0.04173683]\n",
            " [ 0.00056997 -0.05855423  0.01945436 ... -0.01503947 -0.06014021\n",
            "  -0.06122059]\n",
            " ...\n",
            " [ 0.07245281  0.06857903 -0.04508004 ... -0.02142529 -0.00622921\n",
            "  -0.04571254]\n",
            " [ 0.01967357 -0.05130974  0.0628401  ... -0.03714635  0.07161632\n",
            "   0.0370641 ]\n",
            " [ 0.05512607 -0.02233167  0.04043058 ...  0.02383304  0.03273168\n",
            "  -0.02281661]]\n",
            "(784, 300)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlmYDbOPYC-F"
      },
      "source": [
        "###모델 컴파일"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVDUg7vgTec8"
      },
      "source": [
        "손실함수, 옵티마이저, 평가지표를 지정하는 단계.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSePR4lQTUI4"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWN11MzkYC3b"
      },
      "source": [
        "###모델 훈련과 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY_bxirgTqHh",
        "outputId": "25e5dbc2-b71f-47d5-892a-35ce820a7513"
      },
      "source": [
        "#3분 소요\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid), n_jobs=-1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7304 - accuracy: 0.7571 - val_loss: 0.5133 - val_accuracy: 0.8276\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4895 - accuracy: 0.8308 - val_loss: 0.4594 - val_accuracy: 0.8418\n",
            "Epoch 3/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4414 - accuracy: 0.8449 - val_loss: 0.4255 - val_accuracy: 0.8514\n",
            "Epoch 4/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4137 - accuracy: 0.8554 - val_loss: 0.4297 - val_accuracy: 0.8510\n",
            "Epoch 5/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3914 - accuracy: 0.8614 - val_loss: 0.3983 - val_accuracy: 0.8620\n",
            "Epoch 6/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3761 - accuracy: 0.8669 - val_loss: 0.3746 - val_accuracy: 0.8682\n",
            "Epoch 7/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3627 - accuracy: 0.8718 - val_loss: 0.3565 - val_accuracy: 0.8766\n",
            "Epoch 8/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3500 - accuracy: 0.8756 - val_loss: 0.3725 - val_accuracy: 0.8672\n",
            "Epoch 9/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3402 - accuracy: 0.8785 - val_loss: 0.3589 - val_accuracy: 0.8738\n",
            "Epoch 10/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3304 - accuracy: 0.8803 - val_loss: 0.3500 - val_accuracy: 0.8732\n",
            "Epoch 11/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3216 - accuracy: 0.8843 - val_loss: 0.3439 - val_accuracy: 0.8756\n",
            "Epoch 12/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3132 - accuracy: 0.8877 - val_loss: 0.3341 - val_accuracy: 0.8820\n",
            "Epoch 13/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3058 - accuracy: 0.8900 - val_loss: 0.3489 - val_accuracy: 0.8740\n",
            "Epoch 14/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2990 - accuracy: 0.8926 - val_loss: 0.3273 - val_accuracy: 0.8808\n",
            "Epoch 15/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2931 - accuracy: 0.8944 - val_loss: 0.3165 - val_accuracy: 0.8870\n",
            "Epoch 16/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2863 - accuracy: 0.8970 - val_loss: 0.3260 - val_accuracy: 0.8824\n",
            "Epoch 17/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2813 - accuracy: 0.8975 - val_loss: 0.3417 - val_accuracy: 0.8780\n",
            "Epoch 18/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2753 - accuracy: 0.9014 - val_loss: 0.3415 - val_accuracy: 0.8770\n",
            "Epoch 19/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2700 - accuracy: 0.9029 - val_loss: 0.3145 - val_accuracy: 0.8884\n",
            "Epoch 20/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2652 - accuracy: 0.9045 - val_loss: 0.3415 - val_accuracy: 0.8768\n",
            "Epoch 21/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2598 - accuracy: 0.9058 - val_loss: 0.3179 - val_accuracy: 0.8846\n",
            "Epoch 22/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2556 - accuracy: 0.9079 - val_loss: 0.3037 - val_accuracy: 0.8914\n",
            "Epoch 23/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2502 - accuracy: 0.9099 - val_loss: 0.3037 - val_accuracy: 0.8946\n",
            "Epoch 24/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2474 - accuracy: 0.9097 - val_loss: 0.3098 - val_accuracy: 0.8894\n",
            "Epoch 25/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2424 - accuracy: 0.9132 - val_loss: 0.3078 - val_accuracy: 0.8862\n",
            "Epoch 26/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2388 - accuracy: 0.9134 - val_loss: 0.2928 - val_accuracy: 0.8934\n",
            "Epoch 27/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2355 - accuracy: 0.9151 - val_loss: 0.3258 - val_accuracy: 0.8842\n",
            "Epoch 28/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2300 - accuracy: 0.9161 - val_loss: 0.2993 - val_accuracy: 0.8940\n",
            "Epoch 29/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2266 - accuracy: 0.9193 - val_loss: 0.3186 - val_accuracy: 0.8810\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2235 - accuracy: 0.9195 - val_loss: 0.2989 - val_accuracy: 0.8900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "jSaTUVIbUeKb",
        "outputId": "34e35707-30bb-456d-c90a-040a2cd58ff1"
      },
      "source": [
        "#History 객체에 훈련파라미터와 에포크 정보가 들어있다..\n",
        "#그걸 그래프로 그림..\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0,1)     #수직축의 범위를 0~1로 지정\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD0tpqITVDNg",
        "outputId": "b7703936-882e-4f42-b8ef-fb7444172c92"
      },
      "source": [
        "#검증세트보다 조금 낮은 성능이 나오는게 일반적..\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 58.9150 - accuracy: 0.8585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[58.91498565673828, 0.8585000038146973]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKEjfc9cYCwm"
      },
      "source": [
        "###모델을 사용해 예측을 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fScT0SPgVQVP",
        "outputId": "3ec32112-c94a-4c1f-f4ca-941bb1b1a5f1"
      },
      "source": [
        "#새로운 데이터에 대해서 예측을 한다..\n",
        "#각 클래스일 확률을 반환..\n",
        "\n",
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "zUL6T-vDVjf2",
        "outputId": "5fc4fa2d-3898-41e6-ab7a-e12b7a51faa8"
      },
      "source": [
        "y_pred = model.predict_classes(X_new)\n",
        "print(y_pred)\n",
        "print(np.array(class_names)[y_pred])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a5079844f810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq0pZHgwV9Aj",
        "outputId": "77d44095-0982-4b90-eefd-07389bd48336"
      },
      "source": [
        "y_new = y_test[:3]\n",
        "y_new"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyKGttjdYCkn"
      },
      "source": [
        "##10.2.3 시퀀셜 API를 사용하여 회귀용 다층 퍼셉트론 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLL7PfdqWC7e"
      },
      "source": [
        "화귀에서 달라진 점\n",
        "\n",
        "- 출력층이 활성화 함수가 없는 하나의 뉴런이다\n",
        "- 손실함수로 RMSE를 사용한다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzfpXEUnWaGW"
      },
      "source": [
        "캘리포니아 주택 가격 데이터셋으로 시도하겠다 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_LNVrX6WX91",
        "outputId": "eca24651-13aa-468d-d1bf-8957d8b10ba8"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "#훈련, 검증, 테스트세트 분리..\n",
        "X_train_full , X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
        "\n",
        "#스케일링까지..\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoALZTmPXn_1",
        "outputId": "3eab73b8-4ac4-4377-fa0f-a979930604ca"
      },
      "source": [
        "#11610개의 샘플, 8개의 특성\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11610, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwZ6AEu0WK1A",
        "outputId": "67f1c84a-d8e8-49a1-9e37-c35ae3b5c2a3"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),        #8개의 특성이 입력된다..\n",
        "        keras.layers.Dense(1)       #출력층, 원하는 값의 수만큼 뉴런 생성\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "\n",
        "X_new = X_test[:3]      #새로운 샘플이라고 생각..\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.9391 - val_loss: 0.5433\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5178 - val_loss: 0.4500\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4638 - val_loss: 0.4175\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4303 - val_loss: 0.3935\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4111 - val_loss: 0.3961\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.3788\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3936 - val_loss: 0.3687\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3872 - val_loss: 0.3672\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3892 - val_loss: 0.3692\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.3639\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3820 - val_loss: 0.3558\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3538\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3849 - val_loss: 0.3593\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3503\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.3488\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3766 - val_loss: 0.3426\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3433\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3604 - val_loss: 0.3410\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3577 - val_loss: 0.3382\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3551 - val_loss: 0.3394\n",
            "162/162 [==============================] - 0s 988us/step - loss: 0.3704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMUW_6TZYfWe",
        "outputId": "8e8f563a-0388-4899-b8e4-6eb5522735f6"
      },
      "source": [
        "#손실함수 값..\n",
        "\n",
        "mse_test"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3704068064689636"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acnSDgSrYmib",
        "outputId": "7084e787-28e0-443e-a94f-914240e3c0c0"
      },
      "source": [
        "#예측 값..\n",
        "\n",
        "y_pred"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.336951 ],\n",
              "       [1.2654414],\n",
              "       [2.6421933]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxz9imRHYChI"
      },
      "source": [
        "##10.2.4 함수형 API를 사용해 복잡한 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq5xSVQFYxW8"
      },
      "source": [
        "입력의 일부 또는 전체가 출력층에 바로 연결됨..\n",
        "\n",
        "- 은닉층을 모두 통과하는 입력은 **복잡한 패턴**을 학습한다\n",
        "- 바로 출력층에 연결되는 입력은 **간단한 규칙**을 학습한다.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJaucPc2ZAvG"
      },
      "source": [
        "#입력 지정..\n",
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
        "\n",
        "#층을 함수라고 생각했을때 input_이 입력이 된다..\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)     \n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "\n",
        "#입력과 은닉층의 결과를 연결하는 연결층..\n",
        "concat = keras.layers.Concatenate()([input_, hidden2])\n",
        "\n",
        "#출력층에 전달..\n",
        "output = keras.layers.Dense(1)(concat)\n",
        "\n",
        "#초기 입력과 최종 출력을 연결해서 모델 생성..\n",
        "model = keras.Model(inputs=[input_], outputs=[output])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28Z9L0ZsbjbO",
        "outputId": "bdb1f606-1b01-459f-bec5-b13573f75dae"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 30)           270         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 30)           930         dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
            "                                                                 dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 1)            39          concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,239\n",
            "Trainable params: 1,239\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktd9E7PhcDq-"
      },
      "source": [
        "컴파일하고 훈련해서 예측하기.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7eFR8UXcB3C",
        "outputId": "94bd3e5b-7429-463d-ad45-ba7900eeeadc"
      },
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.1612 - val_loss: 0.7674\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.7281 - val_loss: 0.6537\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6550 - val_loss: 0.6138\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6081 - val_loss: 0.5761\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5753 - val_loss: 0.5449\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5503 - val_loss: 0.5219\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5301 - val_loss: 0.5064\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5139 - val_loss: 0.4917\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4975 - val_loss: 0.4733\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4893 - val_loss: 0.4635\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4773 - val_loss: 0.4659\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4681 - val_loss: 0.4454\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.4426\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4562 - val_loss: 0.4341\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4550 - val_loss: 0.4326\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4456 - val_loss: 0.4258\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4403 - val_loss: 0.4196\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4374 - val_loss: 0.4163\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4320 - val_loss: 0.4146\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4279 - val_loss: 0.4125\n",
            "162/162 [==============================] - 0s 994us/step - loss: 0.4415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-8ePzUDcNjc"
      },
      "source": [
        "두 경로에 다른 입력 특성을 전달하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgjYh6IscKby"
      },
      "source": [
        "#입력을 따로 지정..\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "\n",
        "#은닉층에는 input_B만 전달\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "\n",
        "#input_A와 은닉층의 결과 합침..\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "#출력층에 전달..\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
        "\n",
        "#두개의 입력을 구분해서 리스트로 전달..\n",
        "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBSISALccj8-",
        "outputId": "3d45795e-f275-4468-a908-7bb9d3ebc302"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "deep_input (InputLayer)         [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 30)           210         deep_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "wide_input (InputLayer)         [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 30)           930         dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 35)           0           wide_input[0][0]                 \n",
            "                                                                 dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            36          concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,176\n",
            "Trainable params: 1,176\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRL3UTUfcyL6"
      },
      "source": [
        "컴파일, 훈련, 예측 할때도 모두 입력을 다르게 해줘야함..\n",
        "\n",
        "- 모델의 입력에 맞도록!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9xs8ascxwj",
        "outputId": "0eb9009a-8ece-4b23-809c-3b5d04c13ce7"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "\n",
        "#전체 특성이 8개였음..\n",
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
        "\n",
        "#새로운 데이터 지정..\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
        "\n",
        "#훈련(입력을 다르게 넣어줌..)\n",
        "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
        "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
        "\n",
        "#예측할 때도..\n",
        "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
        "y_pred = model.predict((X_new_A, X_new_B))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.5693 - val_loss: 1.1597\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.8942 - val_loss: 0.7757\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.7177 - val_loss: 0.6729\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.6500 - val_loss: 0.6197\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6092 - val_loss: 0.5827\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5805 - val_loss: 0.5559\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5579 - val_loss: 0.5343\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5395 - val_loss: 0.5160\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5252 - val_loss: 0.5023\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5127 - val_loss: 0.4899\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5018 - val_loss: 0.4795\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4921 - val_loss: 0.4701\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4844 - val_loss: 0.4616\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4769 - val_loss: 0.4543\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4704 - val_loss: 0.4482\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4646 - val_loss: 0.4420\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4583 - val_loss: 0.4366\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4532 - val_loss: 0.4337\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4492 - val_loss: 0.4285\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4441 - val_loss: 0.4229\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.4515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTT1S4LYdiQ7",
        "outputId": "ebb2765e-4705-453e-fd64-015317fdfe17"
      },
      "source": [
        "mse_test"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45145151019096375"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG-egLgodjgl",
        "outputId": "17f1e39e-26ce-45cd-a6ae-8505ccd96926"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4676446],\n",
              "       [1.644115 ],\n",
              "       [2.6696086]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9sJ_NGwds1N"
      },
      "source": [
        "보조출력을 추가하기!!\n",
        "\n",
        "- 은닉층의 결과만 따로 출력하고 싶다..\n",
        "- 각 출력은 자신만의 손실 함수가 필요하다..\n",
        "    - 따로 컴파일단계에서 지정해 줘야한다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8fO7Jqhd3BH"
      },
      "source": [
        "#위와 동일..\n",
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\n",
        "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
        "\n",
        "#보조 출력 추가.. 두번째 은닉층의 결과를 받아옴..\n",
        "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
        "\n",
        "#모델 생성 시 출력에도 추가..\n",
        "model = keras.models.Model(inputs=[input_A, input_B],\n",
        "                           outputs=[output, aux_output])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NHAOuzPd7My",
        "outputId": "8791d2a7-6318-4454-b772-b1c8fc310fe6"
      },
      "source": [
        "#각 출력을 위한 손실함수를 리스트로 전달..\n",
        "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], \n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-SgAjp8ePGL",
        "outputId": "353b92be-ac4a-4eb2-adc7-a6071d20e4fb"
      },
      "source": [
        "#훈련..\n",
        "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
        "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.7520 - main_output_loss: 2.6143 - aux_output_loss: 3.9911 - val_loss: 1.1841 - val_main_output_loss: 1.0027 - val_aux_output_loss: 2.8165\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.9524 - main_output_loss: 0.7945 - aux_output_loss: 2.3736 - val_loss: 0.7960 - val_main_output_loss: 0.6668 - val_aux_output_loss: 1.9588\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7380 - main_output_loss: 0.6252 - aux_output_loss: 1.7530 - val_loss: 0.6771 - val_main_output_loss: 0.5829 - val_aux_output_loss: 1.5243\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6549 - main_output_loss: 0.5710 - aux_output_loss: 1.4098 - val_loss: 0.6157 - val_main_output_loss: 0.5417 - val_aux_output_loss: 1.2822\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6068 - main_output_loss: 0.5395 - aux_output_loss: 1.2129 - val_loss: 0.5756 - val_main_output_loss: 0.5141 - val_aux_output_loss: 1.1290\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5761 - main_output_loss: 0.5188 - aux_output_loss: 1.0914 - val_loss: 0.5488 - val_main_output_loss: 0.4942 - val_aux_output_loss: 1.0396\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5545 - main_output_loss: 0.5032 - aux_output_loss: 1.0158 - val_loss: 0.5310 - val_main_output_loss: 0.4813 - val_aux_output_loss: 0.9774\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5380 - main_output_loss: 0.4909 - aux_output_loss: 0.9611 - val_loss: 0.5157 - val_main_output_loss: 0.4686 - val_aux_output_loss: 0.9391\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5251 - main_output_loss: 0.4812 - aux_output_loss: 0.9210 - val_loss: 0.5052 - val_main_output_loss: 0.4600 - val_aux_output_loss: 0.9121\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5160 - main_output_loss: 0.4739 - aux_output_loss: 0.8950 - val_loss: 0.4986 - val_main_output_loss: 0.4560 - val_aux_output_loss: 0.8814\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5084 - main_output_loss: 0.4685 - aux_output_loss: 0.8670 - val_loss: 0.4886 - val_main_output_loss: 0.4466 - val_aux_output_loss: 0.8658\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5018 - main_output_loss: 0.4633 - aux_output_loss: 0.8478 - val_loss: 0.4824 - val_main_output_loss: 0.4419 - val_aux_output_loss: 0.8471\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4960 - main_output_loss: 0.4588 - aux_output_loss: 0.8308 - val_loss: 0.4776 - val_main_output_loss: 0.4382 - val_aux_output_loss: 0.8314\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4905 - main_output_loss: 0.4545 - aux_output_loss: 0.8143 - val_loss: 0.4718 - val_main_output_loss: 0.4333 - val_aux_output_loss: 0.8181\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4860 - main_output_loss: 0.4509 - aux_output_loss: 0.8017 - val_loss: 0.4697 - val_main_output_loss: 0.4327 - val_aux_output_loss: 0.8032\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4819 - main_output_loss: 0.4480 - aux_output_loss: 0.7870 - val_loss: 0.4637 - val_main_output_loss: 0.4270 - val_aux_output_loss: 0.7942\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4783 - main_output_loss: 0.4452 - aux_output_loss: 0.7768 - val_loss: 0.4604 - val_main_output_loss: 0.4246 - val_aux_output_loss: 0.7825\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4746 - main_output_loss: 0.4423 - aux_output_loss: 0.7656 - val_loss: 0.4575 - val_main_output_loss: 0.4227 - val_aux_output_loss: 0.7708\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4714 - main_output_loss: 0.4398 - aux_output_loss: 0.7553 - val_loss: 0.4549 - val_main_output_loss: 0.4208 - val_aux_output_loss: 0.7615\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4681 - main_output_loss: 0.4373 - aux_output_loss: 0.7451 - val_loss: 0.4520 - val_main_output_loss: 0.4186 - val_aux_output_loss: 0.7527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQYTS-kHeSk4",
        "outputId": "1d2b60bd-654e-41d1-b230-2a0249c4c499"
      },
      "source": [
        "#evaluate()메서드 수행 시 개별 손실과 함께 총 손실도 반홛..\n",
        "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])\n",
        "\n",
        "#예측도 각 출력에 대한 예측을 반환..\n",
        "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162/162 [==============================] - 0s 1ms/step - loss: 0.4836 - main_output_loss: 0.4498 - aux_output_loss: 0.7876\n",
            "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f212b153200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFzwMi9gei13",
        "outputId": "dd7838af-c9ee-4572-9440-c9d5d67e1338"
      },
      "source": [
        "print(total_loss, main_loss, aux_loss)\n",
        "print(y_pred_main)\n",
        "print(y_pred_aux)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.48357605934143066 0.44979196786880493 0.7876317501068115\n",
            "[[1.483877 ]\n",
            " [1.6635251]\n",
            " [2.5343146]]\n",
            "[[2.0272222]\n",
            " [2.1890395]\n",
            " [2.1293535]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDVRWyISYCQO"
      },
      "source": [
        "##10.2.5 서브클래싱 API로 동적 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDtZifP9ey1B"
      },
      "source": [
        "- 위의 시퀀셜 API, 함수형 API는 모두 선언적이다..\n",
        "- 정적인 그래프이다..\n",
        "- 장점이 많다..\n",
        "    - 저장, 복사, 공유가 쉽다..\n",
        "    - 모델의 구조를 분석하기 쉽다..\n",
        "- 단점이 될 수도 있는데, 좀 딱딱한 느김이다..\n",
        "- 서브클래싱 API를 사용하면 더 자유로워진다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGuR5I7IgxrS"
      },
      "source": [
        "**서브클래싱 API 분석**\n",
        "\n",
        "- 장점\n",
        "    - call()메서드 안에서 어떠한 계산도 사용할 수 있다..\n",
        "- 단점\n",
        "    - 분석이 어렵다..\n",
        "    - 층 간의 연결 정보를 얻기 어렵다.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3beSxn9fGl7"
      },
      "source": [
        "class WideAndDeepModel(keras.Model):\n",
        "    #생성자에서 층 구성..\n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)  #표준 매개변수를 처리합니다\n",
        "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
        "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
        "        self.main_output = keras.layers.Dense(1)\n",
        "        self.aux_output = keras.layers.Dense(1)\n",
        "\n",
        "    #call메서드에서 수행하려는 연산을 기술..\n",
        "    def call(self, inputs):\n",
        "        input_A, input_B = inputs   #인자로 받은 입력..\n",
        "\n",
        "        #은닉층 구성하고 input_B 특성만 전달..\n",
        "        hidden1 = self.hidden1(input_B)\n",
        "        hidden2 = self.hidden2(hidden_1)\n",
        "\n",
        "        #연결층에서 연결..\n",
        "        concat = keras.layers.concatenate([input_A, hidden2])\n",
        "\n",
        "        #메인과 보조출력 추가..\n",
        "        main_output = self.main_output(concat)\n",
        "        aux_output = self.aux_output(hidden2)\n",
        "        return main_output, aux_output\n",
        "\n",
        "model = WideAndDeepModel()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqicIjTYCMl"
      },
      "source": [
        "##10.2.6 모델 저장과 복원"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpY8JJ7tg-lM",
        "outputId": "3d78575a-9435-4a68-c3cc-72b2e4c29754"
      },
      "source": [
        "#모델 생성..\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "#모델 컴파일..\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
        "\n",
        "#훈련 및 예측..\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.5656 - val_loss: 1.0722\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.9043 - val_loss: 0.7302\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.6868 - val_loss: 0.6435\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6266 - val_loss: 0.5967\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5897 - val_loss: 0.5626\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5608 - val_loss: 0.5360\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5391 - val_loss: 0.5156\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5204 - val_loss: 0.4982\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5041 - val_loss: 0.4820\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4914 - val_loss: 0.4698\n",
            "162/162 [==============================] - 0s 976us/step - loss: 0.4894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpLg7XurjzII"
      },
      "source": [
        "#모델 저장\n",
        "\n",
        "model.save(\"my_keras_model.h5\")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxlxpK0Hj3hp"
      },
      "source": [
        "#불러오기\n",
        "\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-04b3v9kDCn",
        "outputId": "f02292bb-e136-4f9a-dade-ba6a2b94cf7e"
      },
      "source": [
        "#가중치 저장 및 불러오기도 가능\n",
        "\n",
        "model.save_weights(\"my_keras_weights.ckpt\")\n",
        "model.load_weights(\"my_keras_weights.ckpt\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2122798fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqxLjejjYCJN"
      },
      "source": [
        "##10.2.7 콜백 사용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noX3hthtkQTK"
      },
      "source": [
        "훈련과정에서 일정한 간격으로 체크포인트를 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6RUJvhHkU99"
      },
      "source": [
        "#모델 생성..\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "#모델 컴파일..\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70WeE5hfk79s"
      },
      "source": [
        "체크포인트 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8qYRRa7km9x",
        "outputId": "e6d57d83-5d02-4380-e838-92097cc28fd5"
      },
      "source": [
        "#최상의 검증 세트 점수에서만 모델 저장..\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb])\n",
        "\n",
        "#최상의 모델로 롤백\n",
        "model = keras.models.load_model(\"my_keras_model.h5\")\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.0136 - val_loss: 0.8766\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7829 - val_loss: 0.6944\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.6754 - val_loss: 0.6333\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6314 - val_loss: 0.5971\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5992 - val_loss: 0.5677\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5726 - val_loss: 0.5451\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5524 - val_loss: 0.5256\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5352 - val_loss: 0.5100\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5205 - val_loss: 0.4967\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5080 - val_loss: 0.4845\n",
            "162/162 [==============================] - 0s 904us/step - loss: 0.5268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XOXm4GXldRC"
      },
      "source": [
        "**조기종료 콜백 사용**\n",
        "\n",
        "- 에포크의 제한이 없어진다..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLU6j3EQlhNK",
        "outputId": "b0e729d7-66d3-4ae8-e7d3-cba21199debd"
      },
      "source": [
        "#조기종료 콜백 객체 생성\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "#훈련..\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4970 - val_loss: 0.4748\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4876 - val_loss: 0.4651\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4783 - val_loss: 0.4564\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4706 - val_loss: 0.4491\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4635 - val_loss: 0.4419\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4565 - val_loss: 0.4354\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 0.4297\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4448 - val_loss: 0.4235\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4394 - val_loss: 0.4182\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4341 - val_loss: 0.4129\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4084\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4248 - val_loss: 0.4040\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4206 - val_loss: 0.3997\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4169 - val_loss: 0.3969\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4130 - val_loss: 0.3927\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4093 - val_loss: 0.3914\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4064 - val_loss: 0.3882\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4033 - val_loss: 0.3846\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4004 - val_loss: 0.3818\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3978 - val_loss: 0.3790\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3951 - val_loss: 0.3763\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3929 - val_loss: 0.3745\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3909 - val_loss: 0.3727\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3889 - val_loss: 0.3709\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3870 - val_loss: 0.3700\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3855 - val_loss: 0.3679\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3838 - val_loss: 0.3669\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3824 - val_loss: 0.3655\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3809 - val_loss: 0.3641\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3796 - val_loss: 0.3629\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3783 - val_loss: 0.3616\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3769 - val_loss: 0.3616\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3761 - val_loss: 0.3604\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3749 - val_loss: 0.3597\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3736 - val_loss: 0.3580\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3728 - val_loss: 0.3571\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3719 - val_loss: 0.3560\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3705 - val_loss: 0.3557\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3698 - val_loss: 0.3547\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3687 - val_loss: 0.3550\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3682 - val_loss: 0.3529\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3672 - val_loss: 0.3523\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3664 - val_loss: 0.3513\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3656 - val_loss: 0.3511\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3648 - val_loss: 0.3502\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3640 - val_loss: 0.3491\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3633 - val_loss: 0.3486\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3625 - val_loss: 0.3482\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3619 - val_loss: 0.3473\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3611 - val_loss: 0.3467\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3606 - val_loss: 0.3461\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3599 - val_loss: 0.3462\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3592 - val_loss: 0.3446\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3587 - val_loss: 0.3440\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3579 - val_loss: 0.3436\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3573 - val_loss: 0.3428\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3568 - val_loss: 0.3423\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3561 - val_loss: 0.3415\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3425\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3550 - val_loss: 0.3411\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3544 - val_loss: 0.3405\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3537 - val_loss: 0.3398\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3532 - val_loss: 0.3392\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3524 - val_loss: 0.3385\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.3377\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3515 - val_loss: 0.3373\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3509 - val_loss: 0.3364\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3498 - val_loss: 0.3360\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3496 - val_loss: 0.3369\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3492 - val_loss: 0.3356\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3485 - val_loss: 0.3354\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3480 - val_loss: 0.3344\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3472 - val_loss: 0.3336\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3469 - val_loss: 0.3331\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3463 - val_loss: 0.3327\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3457 - val_loss: 0.3329\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3451 - val_loss: 0.3323\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3446 - val_loss: 0.3313\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3440 - val_loss: 0.3308\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3435 - val_loss: 0.3305\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3431 - val_loss: 0.3300\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3298\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3419 - val_loss: 0.3292\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3413 - val_loss: 0.3290\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3408 - val_loss: 0.3282\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3403 - val_loss: 0.3274\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3398 - val_loss: 0.3276\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3394 - val_loss: 0.3263\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3388 - val_loss: 0.3259\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3382 - val_loss: 0.3256\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3377 - val_loss: 0.3251\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3373 - val_loss: 0.3250\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.3237\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3363 - val_loss: 0.3233\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3357 - val_loss: 0.3230\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3353 - val_loss: 0.3226\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3347 - val_loss: 0.3217\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3343 - val_loss: 0.3222\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3338 - val_loss: 0.3209\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3332 - val_loss: 0.3203\n",
            "162/162 [==============================] - 0s 906us/step - loss: 0.3532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acxCxnxXmBTW"
      },
      "source": [
        "사용자 정의 콜백도 만들 수 있다..\n",
        "\n",
        "- 훈련하는 동안 검증손실과 훈련손실의 비율을 출력한다..\n",
        "- 과대적합 방지 용도.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbY01M2VmAU4"
      },
      "source": [
        "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxDcDo9Smh42",
        "outputId": "88f58c56-c497-4de2-f069-d492263fc4aa"
      },
      "source": [
        "#이렇게 전달 가능..\n",
        "\n",
        "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[val_train_ratio_cb])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3323 - val_loss: 0.3197\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3316 - val_loss: 0.3194\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3313 - val_loss: 0.3186\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3306 - val_loss: 0.3189\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3304 - val_loss: 0.3175\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3300 - val_loss: 0.3178\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3294 - val_loss: 0.3172\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3174\n",
            "\n",
            "val/train: 0.97\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3287 - val_loss: 0.3164\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3161\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3278 - val_loss: 0.3152\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3273 - val_loss: 0.3159\n",
            "\n",
            "val/train: 0.97\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3272 - val_loss: 0.3144\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3265 - val_loss: 0.3149\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3263 - val_loss: 0.3138\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3260 - val_loss: 0.3135\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3256 - val_loss: 0.3131\n",
            "\n",
            "val/train: 0.96\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3248 - val_loss: 0.3135\n",
            "\n",
            "val/train: 0.97\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3247 - val_loss: 0.3133\n",
            "\n",
            "val/train: 0.97\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3244 - val_loss: 0.3122\n",
            "\n",
            "val/train: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByFPCVFjYCF9"
      },
      "source": [
        "##10.2.8 텐서보드를 사용해 시각화하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAc3thkxYCCj"
      },
      "source": [
        "#10.3 신경망 하이퍼파라미터 튜닝하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuTvj5o_naun"
      },
      "source": [
        "- 신경망이 유연할수록, 트레이드오프로 조정할 하이퍼파라미터가 많다는 단점도 존재한다..\n",
        "\n",
        "- 랜덤서치나 그리드서치를 조진다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZXDewvAn336"
      },
      "source": [
        "build_model()메서드 생성..\n",
        "\n",
        "- 시퀀셜 모델 생성\n",
        "- n_hidden만큼의 은닉층 생성..\n",
        "- 은닉층의 뉴런 수는 n_neurons.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT3_IyGMnrD3"
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    #시퀀셜 모델 생성..\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
        "    \n",
        "    #n_hidden 만큼의 은닉층 추가\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    #옵티마이저 지정..\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6no8HZu7oQrm"
      },
      "source": [
        "#기본 build_model으로 모델 생성..\n",
        "\n",
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Zbk4ysoaHv",
        "outputId": "4def2f32-5c1c-401b-cb20-2342a4fbbe9d"
      },
      "source": [
        "#콜백 지정해서 훈련..\n",
        "\n",
        "keras_reg.fit(X_train, y_train, epochs=100,\n",
        "              validation_data=(X_valid, y_valid),\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.2093 - val_loss: 0.6586\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.6345 - val_loss: 0.5802\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5721 - val_loss: 0.5282\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5383 - val_loss: 0.4963\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5018 - val_loss: 0.4736\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4820 - val_loss: 0.4593\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4681 - val_loss: 0.4438\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4561 - val_loss: 0.4325\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4461 - val_loss: 0.4206\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4385 - val_loss: 0.4157\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4323 - val_loss: 0.4062\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.4029\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4165 - val_loss: 0.3960\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4137 - val_loss: 0.3925\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4076 - val_loss: 0.3874\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4045 - val_loss: 0.3844\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4007 - val_loss: 0.3820\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3981 - val_loss: 0.3783\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.3764\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3933 - val_loss: 0.3750\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.3733\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3896 - val_loss: 0.3718\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3875 - val_loss: 0.3719\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3860 - val_loss: 0.3696\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3843 - val_loss: 0.3692\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3826 - val_loss: 0.3687\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.3656\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3803 - val_loss: 0.3646\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3646\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3629\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3619\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3758 - val_loss: 0.3604\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3763 - val_loss: 0.3605\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3729 - val_loss: 0.3586\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.3592\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3713 - val_loss: 0.3582\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3706 - val_loss: 0.3554\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3697 - val_loss: 0.3554\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.3547\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3669 - val_loss: 0.3543\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3709 - val_loss: 0.3539\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3651 - val_loss: 0.3515\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3513\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3647 - val_loss: 0.3498\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3638 - val_loss: 0.3498\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.3493\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3625 - val_loss: 0.3475\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3599 - val_loss: 0.3463\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3596 - val_loss: 0.3468\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3588 - val_loss: 0.3454\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.3445\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3584 - val_loss: 0.3448\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3567 - val_loss: 0.3457\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3578 - val_loss: 0.3424\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4205 - val_loss: 0.3423\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3413\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3569 - val_loss: 0.3397\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3541 - val_loss: 0.3398\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3532 - val_loss: 0.3400\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3415\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3566 - val_loss: 0.3382\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3378\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3499 - val_loss: 0.3370\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3360\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3498 - val_loss: 0.3357\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.3359\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3475 - val_loss: 0.3375\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3497 - val_loss: 0.3344\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3463 - val_loss: 0.3333\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3632 - val_loss: 0.3334\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3458 - val_loss: 0.3332\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3621 - val_loss: 0.3337\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3453 - val_loss: 0.3326\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3444 - val_loss: 0.3311\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3453 - val_loss: 0.3315\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3586 - val_loss: 0.3312\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3430 - val_loss: 0.3315\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3445 - val_loss: 0.3299\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3438 - val_loss: 0.3318\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3417 - val_loss: 0.3293\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.3311\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3415 - val_loss: 0.3294\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3421 - val_loss: 0.3295\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3401 - val_loss: 0.3285\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3418 - val_loss: 0.3316\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3443 - val_loss: 0.3273\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3586 - val_loss: 0.3274\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3397 - val_loss: 0.3271\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.3262\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3387 - val_loss: 0.3273\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3380 - val_loss: 0.3256\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3395 - val_loss: 0.3266\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3370 - val_loss: 0.3257\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3579 - val_loss: 0.3295\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3500 - val_loss: 0.3261\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3379 - val_loss: 0.3348\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3363 - val_loss: 0.3253\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3420 - val_loss: 0.3257\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4739 - val_loss: 0.3244\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3364 - val_loss: 0.3257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f212f4f1310>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd_KZ6tkotpE",
        "outputId": "d3df659e-5a6a-48e9-b253-6c4d89b2eee6"
      },
      "source": [
        "#점수 확인..\n",
        "mse_test = keras_reg.score(X_test, y_test)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162/162 [==============================] - 0s 1ms/step - loss: 0.4172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aOY221RosCb"
      },
      "source": [
        "#예측 확인..\n",
        "#보류..\n",
        "\n",
        "y_pred = keras_reg.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LZUqfZnpTwS",
        "outputId": "fd57d423-fa96-4ad0-a9c5-38a913246fea"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.3024249, 1.2841678, 2.4486275], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ng-Fvo2o307"
      },
      "source": [
        "**랜덤 서치**\n",
        "\n",
        "- 오류 해결을 위한 .tolist(), .rvs(1000).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-3l0XKXo-cA",
        "outputId": "1d96709e-ff89-4354-d1b8-7315b45ebb0c"
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#탐색할 하이퍼파라미터..\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(),\n",
        "}\n",
        "\n",
        "#랜덤서치 객체.. cv가 지정되므로 X_valid와 y_valid는 조기종료에만 사용됨..\n",
        "#n_jobs=-1하면 오류뜸..\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs,\n",
        "                                   n_iter=10, cv=3, verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_valid, y_valid),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] n_neurons=83, n_hidden=1, learning_rate=0.0028673928684083967 ...\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 1.5072 - val_loss: 0.6960\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6569 - val_loss: 0.6253\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6040 - val_loss: 0.5810\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.5456\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5374 - val_loss: 0.5237\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5364 - val_loss: 0.5191\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.4645\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.4916\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5617 - val_loss: 0.4438\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5089 - val_loss: 0.4705\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4529 - val_loss: 0.4322\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4444 - val_loss: 0.4389\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4361 - val_loss: 0.4168\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4320 - val_loss: 0.4345\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4273 - val_loss: 0.4109\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4207 - val_loss: 0.4062\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4173 - val_loss: 0.4018\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4133 - val_loss: 0.3963\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4105 - val_loss: 0.3942\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4060 - val_loss: 0.3958\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4044 - val_loss: 0.3894\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4013 - val_loss: 0.3878\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3986 - val_loss: 0.3856\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3967 - val_loss: 0.3840\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.3807\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3925 - val_loss: 0.3791\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3918 - val_loss: 0.3809\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3901 - val_loss: 0.3745\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 0.3740\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.3732\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3844 - val_loss: 0.3698\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.3683\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3820 - val_loss: 0.3666\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3809 - val_loss: 0.3678\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3799 - val_loss: 0.3644\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3787 - val_loss: 0.3637\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3781 - val_loss: 0.3628\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3774 - val_loss: 0.3627\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3752 - val_loss: 0.3609\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.3605\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3738 - val_loss: 0.3585\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3731 - val_loss: 0.3581\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3586\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3713 - val_loss: 0.3560\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3701 - val_loss: 0.3572\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3698 - val_loss: 0.3562\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3688 - val_loss: 0.3549\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3684 - val_loss: 0.3546\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.3550\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3667 - val_loss: 0.3569\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3812 - val_loss: 0.3526\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.3523\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3644 - val_loss: 0.3500\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3637 - val_loss: 0.3525\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3638 - val_loss: 0.3502\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3631 - val_loss: 0.3495\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3616 - val_loss: 0.3493\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3615 - val_loss: 0.3483\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.3562\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3605 - val_loss: 0.3494\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3593 - val_loss: 0.3456\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3485\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3591 - val_loss: 0.3516\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3657 - val_loss: 0.3456\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3590 - val_loss: 0.3458\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3567 - val_loss: 0.3433\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3558 - val_loss: 0.3439\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3561 - val_loss: 0.3422\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3556 - val_loss: 0.3414\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3427\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3534 - val_loss: 0.3412\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3535 - val_loss: 0.3407\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.3394\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3538 - val_loss: 0.3481\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3391\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3514 - val_loss: 0.3416\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3513 - val_loss: 0.3373\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.3400\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3521 - val_loss: 0.3395\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3525 - val_loss: 0.3455\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3363\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.3406\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3516 - val_loss: 0.3371\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3418\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3561 - val_loss: 0.3359\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3610 - val_loss: 0.3382\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3618 - val_loss: 0.3344\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3697 - val_loss: 0.3416\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3675 - val_loss: 0.3367\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3657 - val_loss: 0.3360\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3624 - val_loss: 0.3383\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3599 - val_loss: 0.3348\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3342\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3487 - val_loss: 0.3387\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3466 - val_loss: 0.3322\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3445 - val_loss: 0.3324\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.3319\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3448 - val_loss: 0.3319\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3418 - val_loss: 0.3300\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3417 - val_loss: 0.3316\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3557\n",
            "[CV]  n_neurons=83, n_hidden=1, learning_rate=0.0028673928684083967, total=  41.4s\n",
            "[CV] n_neurons=83, n_hidden=1, learning_rate=0.0028673928684083967 ...\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.4s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "242/242 [==============================] - 1s 2ms/step - loss: 1.5318 - val_loss: 0.6494\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7072 - val_loss: 0.5853\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 0.5345\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5415 - val_loss: 0.4987\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5107 - val_loss: 0.4738\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4896 - val_loss: 0.4558\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4740 - val_loss: 0.4433\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4631 - val_loss: 0.4338\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4537 - val_loss: 0.4273\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4191\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4396 - val_loss: 0.4154\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4351 - val_loss: 0.4088\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.4042\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4259 - val_loss: 0.4005\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4220 - val_loss: 0.3972\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4185 - val_loss: 0.3943\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4152 - val_loss: 0.3913\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4126 - val_loss: 0.3898\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4095 - val_loss: 0.3879\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4067 - val_loss: 0.3852\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4050 - val_loss: 0.3828\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3819\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4003 - val_loss: 0.3792\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3988 - val_loss: 0.3777\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3968 - val_loss: 0.3776\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 0.3750\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3938 - val_loss: 0.3746\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3927 - val_loss: 0.3727\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3911 - val_loss: 0.3715\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3896 - val_loss: 0.3705\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3885 - val_loss: 0.3701\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3871 - val_loss: 0.3693\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.3671\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3849 - val_loss: 0.3664\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3655\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3828 - val_loss: 0.3647\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3637\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3808 - val_loss: 0.3637\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3797 - val_loss: 0.3624\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3787 - val_loss: 0.3615\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3775 - val_loss: 0.3609\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3768 - val_loss: 0.3605\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 0.3596\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3750 - val_loss: 0.3589\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3738 - val_loss: 0.3579\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3732 - val_loss: 0.3587\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3723 - val_loss: 0.3574\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3712 - val_loss: 0.3555\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3700 - val_loss: 0.3554\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.3540\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3683 - val_loss: 0.3559\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.3527\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3668 - val_loss: 0.3519\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3664 - val_loss: 0.3510\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3653 - val_loss: 0.3514\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3650 - val_loss: 0.3507\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3637 - val_loss: 0.3515\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3635 - val_loss: 0.3491\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3625 - val_loss: 0.3489\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3500\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3616 - val_loss: 0.3480\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3606 - val_loss: 0.3468\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3599 - val_loss: 0.3463\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.3474\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3455\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3582 - val_loss: 0.3466\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3577 - val_loss: 0.3444\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3574 - val_loss: 0.3445\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3566 - val_loss: 0.3441\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3562 - val_loss: 0.3444\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3557 - val_loss: 0.3430\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3435\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3418\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3432\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3529 - val_loss: 0.3443\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3532 - val_loss: 0.3410\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3409\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3396\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3395\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3509 - val_loss: 0.3393\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3508 - val_loss: 0.3390\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3498 - val_loss: 0.3389\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3496 - val_loss: 0.3376\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 0.3374\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3486 - val_loss: 0.3382\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3481 - val_loss: 0.3371\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3474 - val_loss: 0.3388\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3474 - val_loss: 0.3366\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3471 - val_loss: 0.3356\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3468 - val_loss: 0.3351\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3465 - val_loss: 0.3348\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3453 - val_loss: 0.3355\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3455 - val_loss: 0.3350\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3449 - val_loss: 0.3355\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3444 - val_loss: 0.3334\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3443 - val_loss: 0.3338\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3443 - val_loss: 0.3331\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.3325\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3433 - val_loss: 0.3338\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3429 - val_loss: 0.3326\n",
            "121/121 [==============================] - 0s 984us/step - loss: 0.3590\n",
            "[CV]  n_neurons=83, n_hidden=1, learning_rate=0.0028673928684083967, total=  41.4s\n",
            "[CV] n_neurons=83, n_hidden=1, learning_rate=0.0028673928684083967 ...\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3927 - val_loss: 0.7274\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6830 - val_loss: 0.6107\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6020 - val_loss: 0.5490\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5496 - val_loss: 0.5122\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5173 - val_loss: 0.4843\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4941 - val_loss: 0.4658\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4767 - val_loss: 0.4506\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4653 - val_loss: 0.4399\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4540 - val_loss: 0.4303\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4453 - val_loss: 0.4234\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4173\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4321 - val_loss: 0.4114\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4248 - val_loss: 0.4055\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4201 - val_loss: 0.4018\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4146 - val_loss: 0.3980\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4113 - val_loss: 0.3938\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4071 - val_loss: 0.3908\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4043 - val_loss: 0.3897\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4011 - val_loss: 0.3853\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3986 - val_loss: 0.3845\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3828\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3940 - val_loss: 0.3799\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3917 - val_loss: 0.3775\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3899 - val_loss: 0.3754\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3884 - val_loss: 0.3753\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3892 - val_loss: 0.3722\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3850 - val_loss: 0.3714\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3832 - val_loss: 0.3704\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3718\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.3684\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3796 - val_loss: 0.3664\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3785 - val_loss: 0.3690\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3772 - val_loss: 0.3641\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 0.3638\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3742 - val_loss: 0.3620\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3742 - val_loss: 0.3618\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.3610\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3713 - val_loss: 0.3592\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3709 - val_loss: 0.3595\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3693 - val_loss: 0.3582\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3684 - val_loss: 0.3577\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3674 - val_loss: 0.3566\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3680 - val_loss: 0.3550\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3657 - val_loss: 0.3551\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3661 - val_loss: 0.3541\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3643 - val_loss: 0.3535\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3635 - val_loss: 0.3538\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3627 - val_loss: 0.3530\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3622 - val_loss: 0.3525\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3517\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3604 - val_loss: 0.3505\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3598 - val_loss: 0.3502\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3594 - val_loss: 0.3498\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3587 - val_loss: 0.3540\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3479\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3574 - val_loss: 0.3470\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3569 - val_loss: 0.3475\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3562 - val_loss: 0.3471\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3470\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3452\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3540 - val_loss: 0.3452\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.3453\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3529 - val_loss: 0.3452\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.3432\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3523 - val_loss: 0.3430\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3519 - val_loss: 0.3423\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3509 - val_loss: 0.3421\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3504 - val_loss: 0.3428\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.3417\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 0.3410\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3481 - val_loss: 0.3436\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3486 - val_loss: 0.3402\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3475 - val_loss: 0.3405\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3479 - val_loss: 0.3398\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3463 - val_loss: 0.3395\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3387\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3468 - val_loss: 0.3381\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3457 - val_loss: 0.3381\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 0.3374\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3444 - val_loss: 0.3369\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3443 - val_loss: 0.3366\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3451 - val_loss: 0.3366\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3439 - val_loss: 0.3358\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3429 - val_loss: 0.3358\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3426 - val_loss: 0.3344\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3421 - val_loss: 0.3349\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3422 - val_loss: 0.3341\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3407 - val_loss: 0.3357\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3412 - val_loss: 0.3330\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3406 - val_loss: 0.3330\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3396 - val_loss: 0.3322\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3409 - val_loss: 0.3335\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3394 - val_loss: 0.3319\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3393 - val_loss: 0.3327\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3381 - val_loss: 0.3321\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3381 - val_loss: 0.3307\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.3318\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3380 - val_loss: 0.3308\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3368 - val_loss: 0.3307\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 0.3313\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3573\n",
            "[CV]  n_neurons=83, n_hidden=1, learning_rate=0.0028673928684083967, total= 1.4min\n",
            "[CV] n_neurons=70, n_hidden=0, learning_rate=0.001955563560475647 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 3.3441 - val_loss: 1.3592\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8985 - val_loss: 0.6500\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5846 - val_loss: 0.5440\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5375 - val_loss: 0.5267\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5165\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.5167\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5170\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5189\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5150\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 0.5180\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5121\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5288 - val_loss: 0.5148\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.5142\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5287 - val_loss: 0.5144\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5124\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.5126\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 0.5180\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.5181\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5183\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.5114\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 0.5105\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5137\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5283 - val_loss: 0.5166\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5162\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 0.5099\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5170\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 0.5123\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.5093\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.5158\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5282 - val_loss: 0.5216\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5281 - val_loss: 0.5131\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 0.5122\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 0.5172\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5296 - val_loss: 0.5158\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5100\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5154\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.5185\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5270 - val_loss: 0.5100\n",
            "121/121 [==============================] - 0s 925us/step - loss: 0.5358\n",
            "[CV]  n_neurons=70, n_hidden=0, learning_rate=0.001955563560475647, total=  14.6s\n",
            "[CV] n_neurons=70, n_hidden=0, learning_rate=0.001955563560475647 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 3.8888 - val_loss: 1.6871\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 1.1228 - val_loss: 0.7851\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7095 - val_loss: 0.6313\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6326 - val_loss: 0.5940\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6102 - val_loss: 0.5788\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 0.5716\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.5616\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5530\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 0.5482\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 0.5422\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5643 - val_loss: 0.5377\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5591 - val_loss: 0.5319\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5568 - val_loss: 0.5302\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 0.5246\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5503 - val_loss: 0.5223\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5469 - val_loss: 0.5187\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5453 - val_loss: 0.5170\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 0.5144\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5421 - val_loss: 0.5144\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5122\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5379 - val_loss: 0.5098\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5383 - val_loss: 0.5106\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5370 - val_loss: 0.5095\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5356 - val_loss: 0.5069\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5351 - val_loss: 0.5063\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5075\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5327 - val_loss: 0.5036\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5342 - val_loss: 0.5035\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5340 - val_loss: 0.5043\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5022\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.5038\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.5024\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 0.5031\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.5004\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5021\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5305 - val_loss: 0.4999\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5015\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5008\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5309 - val_loss: 0.5009\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.5011\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.4997\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5020\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.4993\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5002\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5008\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5297 - val_loss: 0.4988\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5309 - val_loss: 0.5012\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4992\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4994\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5293 - val_loss: 0.4981\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5005\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 0.4979\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.4980\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5316 - val_loss: 0.5006\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.4996\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.4981\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.4988\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.5001\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.4978\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4979\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.4981\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4986\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.4985\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4997\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.4999\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.5000\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.4993\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5299 - val_loss: 0.4987\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4986\n",
            "121/121 [==============================] - 0s 930us/step - loss: 0.5501\n",
            "[CV]  n_neurons=70, n_hidden=0, learning_rate=0.001955563560475647, total=  41.4s\n",
            "[CV] n_neurons=70, n_hidden=0, learning_rate=0.001955563560475647 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 2.9286 - val_loss: 1.4700\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0868 - val_loss: 0.8441\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7969 - val_loss: 0.7245\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7256 - val_loss: 0.6824\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6913 - val_loss: 0.6548\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6661 - val_loss: 0.6334\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6458 - val_loss: 0.6155\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6286 - val_loss: 0.5999\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6139 - val_loss: 0.5871\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.6013 - val_loss: 0.5757\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5907 - val_loss: 0.5662\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5817 - val_loss: 0.5583\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5506\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5670 - val_loss: 0.5436\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5614 - val_loss: 0.5397\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5564 - val_loss: 0.5334\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5526 - val_loss: 0.5299\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.5276\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5235\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5434 - val_loss: 0.5214\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5410 - val_loss: 0.5197\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5390 - val_loss: 0.5203\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5375 - val_loss: 0.5165\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5360 - val_loss: 0.5160\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.5166\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5335 - val_loss: 0.5119\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.5158\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 0.5086\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5076\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.5133\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.5102\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 0.5102\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5045\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5291 - val_loss: 0.5075\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.5043\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5111\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5277 - val_loss: 0.5061\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5038\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.5065\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.5031\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.5013\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.5026\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5112\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5265 - val_loss: 0.5042\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.5078\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5030\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5015\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.5015\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 0.4991\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.5087\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.5033\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5258 - val_loss: 0.5048\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 0.5014\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5089\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 0.5095\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.5039\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 0.4998\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.4988\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5264 - val_loss: 0.5039\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5071\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5258 - val_loss: 0.5067\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 0.5024\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5254 - val_loss: 0.5090\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5069\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5057\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 0.4989\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5262 - val_loss: 0.5013\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 0.5021\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5621\n",
            "[CV]  n_neurons=70, n_hidden=0, learning_rate=0.001955563560475647, total=  41.4s\n",
            "[CV] n_neurons=91, n_hidden=3, learning_rate=0.012281399782266571 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 0.7795 - val_loss: 0.5911\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6561 - val_loss: 0.4272\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4380 - val_loss: 0.3789\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6264 - val_loss: 0.3855\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4899 - val_loss: 0.3708\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3711 - val_loss: 0.3602\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3564 - val_loss: 0.3489\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3465 - val_loss: 0.3249\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3367 - val_loss: 0.3339\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.3275\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3250 - val_loss: 0.3164\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3211 - val_loss: 0.3163\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3168 - val_loss: 0.3073\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3116 - val_loss: 0.3003\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3097 - val_loss: 0.3119\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3033 - val_loss: 0.2983\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3010 - val_loss: 0.2952\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2988 - val_loss: 0.3033\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2961 - val_loss: 0.2933\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2940 - val_loss: 0.2930\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2907 - val_loss: 0.2916\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2889 - val_loss: 0.3089\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2882 - val_loss: 0.3017\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2892 - val_loss: 0.2865\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2857 - val_loss: 0.2927\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2840 - val_loss: 0.3008\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2802 - val_loss: 0.2834\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2806 - val_loss: 0.2936\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2808 - val_loss: 0.2877\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2765 - val_loss: 0.2883\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2769 - val_loss: 0.2894\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2754 - val_loss: 0.2804\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2749 - val_loss: 0.2835\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2730 - val_loss: 0.2868\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2714 - val_loss: 0.2895\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2731 - val_loss: 0.2795\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2694 - val_loss: 0.2863\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2687 - val_loss: 0.3015\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2682 - val_loss: 0.2928\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2675 - val_loss: 0.2900\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2654 - val_loss: 0.2809\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2662 - val_loss: 0.2774\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2665 - val_loss: 0.2743\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2645 - val_loss: 0.2712\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2637 - val_loss: 0.2725\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2623 - val_loss: 0.2889\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2629 - val_loss: 0.2854\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2596 - val_loss: 0.3183\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2593 - val_loss: 0.3038\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2600 - val_loss: 0.2770\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2598 - val_loss: 0.2669\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2563 - val_loss: 0.2876\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2545 - val_loss: 0.2733\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2555 - val_loss: 0.2954\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2542 - val_loss: 0.2698\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2540 - val_loss: 0.2855\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2533 - val_loss: 0.2880\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2504 - val_loss: 0.2704\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2503 - val_loss: 0.2743\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2509 - val_loss: 0.2691\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2498 - val_loss: 0.2801\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3050\n",
            "[CV]  n_neurons=91, n_hidden=3, learning_rate=0.012281399782266571, total=  31.4s\n",
            "[CV] n_neurons=91, n_hidden=3, learning_rate=0.012281399782266571 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.8187 - val_loss: 0.4780\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6938 - val_loss: 0.3951\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4165 - val_loss: 0.3687\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3886 - val_loss: 0.3642\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3737 - val_loss: 0.3544\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3611 - val_loss: 0.3645\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3477\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3453 - val_loss: 0.3361\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3419 - val_loss: 0.3365\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3379 - val_loss: 0.3345\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3312 - val_loss: 0.3266\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3300 - val_loss: 0.3204\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3262 - val_loss: 0.3283\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3226 - val_loss: 0.3282\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3204 - val_loss: 0.3163\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3176 - val_loss: 0.3280\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3114 - val_loss: 0.3190\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3095 - val_loss: 0.3072\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3120 - val_loss: 0.3024\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3063 - val_loss: 0.3012\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3034 - val_loss: 0.3280\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3007 - val_loss: 0.3132\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2970 - val_loss: 0.2994\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2978 - val_loss: 0.3032\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2940 - val_loss: 0.3009\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2934 - val_loss: 0.2972\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2914 - val_loss: 0.3051\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2888 - val_loss: 0.3191\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2869 - val_loss: 0.3008\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2873 - val_loss: 0.2891\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2850 - val_loss: 0.2889\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2830 - val_loss: 0.2924\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2814 - val_loss: 0.3012\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2774 - val_loss: 0.2863\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2797 - val_loss: 0.3021\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2767 - val_loss: 0.2915\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2776 - val_loss: 0.2885\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2764 - val_loss: 0.2887\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2717 - val_loss: 0.2924\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2749 - val_loss: 0.2805\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2723 - val_loss: 0.2850\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2697 - val_loss: 0.2985\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2697 - val_loss: 0.2883\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2688 - val_loss: 0.2824\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2686 - val_loss: 0.2806\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2664 - val_loss: 0.2942\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2669 - val_loss: 0.2925\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2653 - val_loss: 0.2865\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2670 - val_loss: 0.2918\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.2789\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2653 - val_loss: 0.2781\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2677 - val_loss: 0.2775\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2617 - val_loss: 0.2823\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2619 - val_loss: 0.2934\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2622 - val_loss: 0.2836\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2573 - val_loss: 0.2816\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.2808\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2585 - val_loss: 0.2883\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2576 - val_loss: 0.2754\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2580 - val_loss: 0.2922\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2543 - val_loss: 0.2944\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2549 - val_loss: 0.2755\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2549 - val_loss: 0.2802\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2523 - val_loss: 0.2943\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2519 - val_loss: 0.2789\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2534 - val_loss: 0.2830\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2530 - val_loss: 0.2812\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2483 - val_loss: 0.2885\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2502 - val_loss: 0.2706\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2488 - val_loss: 0.2757\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2487 - val_loss: 0.2774\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.2718\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2453 - val_loss: 0.2695\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2474 - val_loss: 0.2747\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2445 - val_loss: 0.2890\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2455 - val_loss: 0.2812\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2435 - val_loss: 0.2796\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2446 - val_loss: 0.2983\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2450 - val_loss: 0.2730\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2424 - val_loss: 0.2754\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2414 - val_loss: 0.2728\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2414 - val_loss: 0.2860\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2407 - val_loss: 0.2728\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.2836\n",
            "[CV]  n_neurons=91, n_hidden=3, learning_rate=0.012281399782266571, total=  43.0s\n",
            "[CV] n_neurons=91, n_hidden=3, learning_rate=0.012281399782266571 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.7044 - val_loss: 0.4487\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4411 - val_loss: 0.3969\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3869 - val_loss: 0.3637\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3704 - val_loss: 0.3661\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3596 - val_loss: 0.3694\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3646 - val_loss: 0.3352\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3458 - val_loss: 0.3386\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3393 - val_loss: 0.3415\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3332 - val_loss: 0.3244\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3288 - val_loss: 0.3319\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3250 - val_loss: 0.3237\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3197 - val_loss: 0.3147\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3171 - val_loss: 0.3389\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3125 - val_loss: 0.3232\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3107 - val_loss: 0.3116\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3087 - val_loss: 0.3063\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3034 - val_loss: 0.3067\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3008 - val_loss: 0.3123\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2972 - val_loss: 0.3059\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2948 - val_loss: 0.3039\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2944 - val_loss: 0.3122\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2932 - val_loss: 0.3162\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2931 - val_loss: 0.2974\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2890 - val_loss: 0.2912\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2875 - val_loss: 0.2832\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2835 - val_loss: 0.3016\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2841 - val_loss: 0.3015\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2841 - val_loss: 0.2919\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2786 - val_loss: 0.2889\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2762 - val_loss: 0.3069\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2743 - val_loss: 0.2906\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2738 - val_loss: 0.2948\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2714 - val_loss: 0.2960\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.2772\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2698 - val_loss: 0.2836\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2723 - val_loss: 0.2865\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2728 - val_loss: 0.2832\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2687 - val_loss: 0.2791\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2666 - val_loss: 0.2823\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2649 - val_loss: 0.2997\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2635 - val_loss: 0.2813\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2619 - val_loss: 0.2874\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2614 - val_loss: 0.2783\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.2760\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2575 - val_loss: 0.3574\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2584 - val_loss: 0.2929\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2578 - val_loss: 0.2770\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2549 - val_loss: 0.2814\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2565 - val_loss: 0.2736\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2551 - val_loss: 0.2716\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2546 - val_loss: 0.2950\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2518 - val_loss: 0.2696\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2557 - val_loss: 0.2748\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2624 - val_loss: 0.2714\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2572 - val_loss: 0.2710\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2515 - val_loss: 0.2947\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2512 - val_loss: 0.2859\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2529 - val_loss: 0.2725\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2464 - val_loss: 0.3044\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2464 - val_loss: 0.2807\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.2466 - val_loss: 0.2793\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.2468 - val_loss: 0.2699\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.2825\n",
            "[CV]  n_neurons=91, n_hidden=3, learning_rate=0.012281399782266571, total=  32.4s\n",
            "[CV] n_neurons=44, n_hidden=0, learning_rate=0.015670692545593327 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.8175 - val_loss: 0.6286\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.9800 - val_loss: 5.1606\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 86.7581 - val_loss: 136.2812\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2418.3838 - val_loss: 5036.0439\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 128495.7109 - val_loss: 178524.2031\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 1438881.1250 - val_loss: 7049525.0000\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 188845152.0000 - val_loss: 239906800.0000\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1815608064.0000 - val_loss: 9199655936.0000\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 253862412288.0000 - val_loss: 299464130560.0000\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 7072533446656.0000 - val_loss: 10791598161920.0000\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 208237882245120.0000 - val_loss: 392767343689728.0000\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 2268681719513088.0000\n",
            "[CV]  n_neurons=44, n_hidden=0, learning_rate=0.015670692545593327, total=   4.9s\n",
            "[CV] n_neurons=44, n_hidden=0, learning_rate=0.015670692545593327 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 24s 2ms/step - loss: 4.1995 - val_loss: 1.5074\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 71.9250 - val_loss: 13.8041\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 125.0942 - val_loss: 310.6451\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 4227.3271 - val_loss: 7684.3931\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 965931.8125 - val_loss: 220909.8438\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 24217150.0000 - val_loss: 6526141.0000\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 484971584.0000 - val_loss: 160398944.0000\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1830835840.0000 - val_loss: 2883126784.0000\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 70797508608.0000 - val_loss: 70810648576.0000\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 11100241264640.0000 - val_loss: 1762410889216.0000\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 291420879454208.0000 - val_loss: 45024951664640.0000\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 13742300463104.0000\n",
            "[CV]  n_neurons=44, n_hidden=0, learning_rate=0.015670692545593327, total=  28.4s\n",
            "[CV] n_neurons=44, n_hidden=0, learning_rate=0.015670692545593327 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9770 - val_loss: 0.5640\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5575 - val_loss: 0.5507\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.4942\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.6624\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5466 - val_loss: 0.5027\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5450 - val_loss: 0.5320\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 0.5041\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5650 - val_loss: 0.4960\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5181\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5387 - val_loss: 0.5141\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5633 - val_loss: 0.5068\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5682 - val_loss: 0.5157\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5543 - val_loss: 0.5323\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5607\n",
            "[CV]  n_neurons=44, n_hidden=0, learning_rate=0.015670692545593327, total=   5.8s\n",
            "[CV] n_neurons=11, n_hidden=0, learning_rate=0.0020974132623475437 ...\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 4.7445 - val_loss: 1.6663\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1095 - val_loss: 0.8096\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7016 - val_loss: 0.6597\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6339 - val_loss: 0.6204\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6096 - val_loss: 0.5986\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.5899\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.5801\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5779 - val_loss: 0.5680\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5717 - val_loss: 0.5630\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5660 - val_loss: 0.5572\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5596 - val_loss: 0.5495\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5547 - val_loss: 0.5449\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 0.5439\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5474 - val_loss: 0.5377\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.5366\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5419 - val_loss: 0.5327\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5429 - val_loss: 0.5303\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5398 - val_loss: 0.5321\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 0.5242\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5377 - val_loss: 0.5294\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5203\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5289\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5338 - val_loss: 0.5217\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5330 - val_loss: 0.5229\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 0.5222\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5320 - val_loss: 0.5188\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5174\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5318 - val_loss: 0.5217\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.5140\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5169\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.5154\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5136\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5153\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5129\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5132\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 0.5238\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5160\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5125\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.5182\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.5120\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5299 - val_loss: 0.5169\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5213\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.5185\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5119\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5176\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.5196\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5114\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5093\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5183\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5115\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5122\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5109\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 0.5119\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 0.5158\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 0.5098\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5092\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.5109\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5109\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5285 - val_loss: 0.5110\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 0.5093\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5186\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5178\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.5137\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5199\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5145\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 0.5100\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5359\n",
            "[CV]  n_neurons=11, n_hidden=0, learning_rate=0.0020974132623475437, total=  27.8s\n",
            "[CV] n_neurons=11, n_hidden=0, learning_rate=0.0020974132623475437 ...\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 4.2030 - val_loss: 1.6764\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0838 - val_loss: 0.7793\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6989 - val_loss: 0.6331\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6268 - val_loss: 0.5943\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6065 - val_loss: 0.5789\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5928 - val_loss: 0.5650\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 0.5596\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5775 - val_loss: 0.5504\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5703 - val_loss: 0.5426\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5662 - val_loss: 0.5397\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5600 - val_loss: 0.5323\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5576 - val_loss: 0.5307\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5243\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5509 - val_loss: 0.5221\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5477 - val_loss: 0.5204\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5445 - val_loss: 0.5165\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5437 - val_loss: 0.5166\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5412 - val_loss: 0.5135\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5115\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5381 - val_loss: 0.5096\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5374 - val_loss: 0.5076\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5079\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 0.5074\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5353 - val_loss: 0.5060\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5337 - val_loss: 0.5041\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.5045\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5340 - val_loss: 0.5045\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5330 - val_loss: 0.5038\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 0.5030\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.5014\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5328 - val_loss: 0.5015\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5022\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.4999\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.5008\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.4988\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5320 - val_loss: 0.5009\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.5011\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.4992\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5012\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.5018\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.4987\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5316 - val_loss: 0.5005\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.4984\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 0.5019\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.4999\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5000\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4990\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 0.4998\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5004\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.4992\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5001\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4990\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.4987\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5454\n",
            "[CV]  n_neurons=11, n_hidden=0, learning_rate=0.0020974132623475437, total=  22.4s\n",
            "[CV] n_neurons=11, n_hidden=0, learning_rate=0.0020974132623475437 ...\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 4.3640 - val_loss: 1.5272\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0307 - val_loss: 0.7495\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 0.6238\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6210 - val_loss: 0.5898\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 0.5725\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5861 - val_loss: 0.5604\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.5509\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5673 - val_loss: 0.5433\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5603 - val_loss: 0.5374\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5542 - val_loss: 0.5315\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5492 - val_loss: 0.5279\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5453 - val_loss: 0.5232\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5421 - val_loss: 0.5199\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5390 - val_loss: 0.5163\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5372 - val_loss: 0.5159\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5129\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.5104\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 0.5143\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5107\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5077\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 0.5117\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5067\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5092\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5036\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5274 - val_loss: 0.5081\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.5049\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.5043\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5100\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.5008\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5269 - val_loss: 0.5038\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 0.5005\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5040\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 0.5065\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5067\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 0.5021\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 0.4993\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5001\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5033\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5037\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5048\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.5032\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 0.4992\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 0.5027\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 0.5046\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 0.5007\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 0.5094\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 0.4988\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.4994\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5002\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.4992\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 0.4995\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5066\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 0.4980\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5040\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.5010\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 0.5102\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5101\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.5055\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.5002\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5250 - val_loss: 0.4988\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.5088\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 0.4988\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.4989\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5592\n",
            "[CV]  n_neurons=11, n_hidden=0, learning_rate=0.0020974132623475437, total=  41.4s\n",
            "[CV] n_neurons=89, n_hidden=0, learning_rate=0.014159909259796386 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3592 - val_loss: 0.8251\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.2488 - val_loss: 2.0081\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 48.5117 - val_loss: 57.6943\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1478.3383 - val_loss: 1680.6194\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 20529.2383 - val_loss: 52902.0781\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1245113.5000 - val_loss: 1544395.1250\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 23420212.0000 - val_loss: 46671012.0000\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 671891456.0000 - val_loss: 1402412672.0000\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 42461245440.0000 - val_loss: 42083348480.0000\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 629969518592.0000 - val_loss: 1360869064704.0000\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 14596887805952.0000 - val_loss: 39433411756032.0000\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 228846125187072.0000\n",
            "[CV]  n_neurons=89, n_hidden=0, learning_rate=0.014159909259796386, total=   5.6s\n",
            "[CV] n_neurons=89, n_hidden=0, learning_rate=0.014159909259796386 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.1150 - val_loss: 0.5439\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5859 - val_loss: 0.5662\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.7221\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.4643 - val_loss: 3.6717\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 317.8442 - val_loss: 36.2458\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 298.0912 - val_loss: 458.5070\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2685.8184 - val_loss: 6477.6226\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 636457.8750 - val_loss: 89074.4453\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 12197667.0000 - val_loss: 1042682.3125\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 150987632.0000 - val_loss: 13840875.0000\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 214690688.0000 - val_loss: 184943392.0000\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 54159128.0000\n",
            "[CV]  n_neurons=89, n_hidden=0, learning_rate=0.014159909259796386, total=   5.6s\n",
            "[CV] n_neurons=89, n_hidden=0, learning_rate=0.014159909259796386 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3501 - val_loss: 0.5861\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5595 - val_loss: 0.5238\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5623 - val_loss: 0.5113\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.5028\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5631 - val_loss: 0.5367\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 0.5082\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5644 - val_loss: 0.5077\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 0.5044\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5436 - val_loss: 0.5084\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5779 - val_loss: 0.6543\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.4972\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5418 - val_loss: 0.5007\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 0.5110\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 0.5069\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5772 - val_loss: 0.5034\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 0.5166\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5413 - val_loss: 0.5107\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5507 - val_loss: 0.4967\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5726 - val_loss: 0.5290\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5088\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5453 - val_loss: 0.5143\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5457 - val_loss: 0.5015\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 0.4977\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.4999\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5579 - val_loss: 0.5166\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5605 - val_loss: 0.5096\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5386 - val_loss: 0.4962\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5493 - val_loss: 0.4966\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 0.5087\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5104\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5679 - val_loss: 0.5118\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5552 - val_loss: 0.4967\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5231\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5411 - val_loss: 0.5012\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5531 - val_loss: 0.5121\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5536 - val_loss: 0.5125\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5650 - val_loss: 0.5138\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5559\n",
            "[CV]  n_neurons=89, n_hidden=0, learning_rate=0.014159909259796386, total=  21.1s\n",
            "[CV] n_neurons=19, n_hidden=1, learning_rate=0.00272269786272835 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.8085 - val_loss: 0.7540\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.5791 - val_loss: 0.8582\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.5168 - val_loss: 1.2261\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.1760 - val_loss: 0.5538\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5524 - val_loss: 0.5232\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.5007\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5048 - val_loss: 0.4824\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4899 - val_loss: 0.4714\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4783 - val_loss: 0.4596\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4694 - val_loss: 0.4529\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4616 - val_loss: 0.4452\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4555 - val_loss: 0.4383\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4505 - val_loss: 0.4331\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4461 - val_loss: 0.4290\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4420 - val_loss: 0.4268\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4205\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4354 - val_loss: 0.4188\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4320 - val_loss: 0.4155\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.4109\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4274 - val_loss: 0.4101\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4253 - val_loss: 0.4081\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4232 - val_loss: 0.4047\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4215 - val_loss: 0.4049\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4197 - val_loss: 0.4015\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4183 - val_loss: 0.4016\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4169 - val_loss: 0.3995\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4153 - val_loss: 0.3991\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4142 - val_loss: 0.3981\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4133 - val_loss: 0.3948\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4118 - val_loss: 0.3942\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4112 - val_loss: 0.3925\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4099 - val_loss: 0.3907\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4087 - val_loss: 0.3899\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4081 - val_loss: 0.3908\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4073 - val_loss: 0.3890\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4063 - val_loss: 0.3885\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4053 - val_loss: 0.3876\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4045 - val_loss: 0.3869\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 0.3856\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4032 - val_loss: 0.3846\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4021 - val_loss: 0.3836\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4011 - val_loss: 0.3844\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 0.3826\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.3866\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3988 - val_loss: 0.3825\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3982 - val_loss: 0.3807\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3972 - val_loss: 0.3809\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3967 - val_loss: 0.3777\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 0.3808\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3953 - val_loss: 0.3774\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3942 - val_loss: 0.3789\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3938 - val_loss: 0.3763\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3928 - val_loss: 0.3791\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3927 - val_loss: 0.3735\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3917 - val_loss: 0.3722\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3908 - val_loss: 0.3734\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3905 - val_loss: 0.3721\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3900 - val_loss: 0.3717\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3892 - val_loss: 0.3723\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3885 - val_loss: 0.3701\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3880 - val_loss: 0.3687\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 0.3698\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3865 - val_loss: 0.3707\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3864 - val_loss: 0.3702\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3858 - val_loss: 0.3667\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3853 - val_loss: 0.3681\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 0.3661\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3848 - val_loss: 0.3672\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 0.3658\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.3658\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3832 - val_loss: 0.3636\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.3638\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3823 - val_loss: 0.3629\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.3633\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.3624\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3809 - val_loss: 0.3618\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3806 - val_loss: 0.3626\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3801 - val_loss: 0.3611\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3794 - val_loss: 0.3610\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3790 - val_loss: 0.3616\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3786 - val_loss: 0.3600\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3779 - val_loss: 0.3598\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3774 - val_loss: 0.3587\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3769 - val_loss: 0.3583\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3765 - val_loss: 0.3584\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3761 - val_loss: 0.3579\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 0.3601\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3752 - val_loss: 0.3574\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3568\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3739 - val_loss: 0.3558\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3734 - val_loss: 0.3552\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.3554\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3725 - val_loss: 0.3557\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.3557\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3545\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3708 - val_loss: 0.3593\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3551\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3532\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3698 - val_loss: 0.3539\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3696 - val_loss: 0.3523\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3784\n",
            "[CV]  n_neurons=19, n_hidden=1, learning_rate=0.00272269786272835, total=  47.6s\n",
            "[CV] n_neurons=19, n_hidden=1, learning_rate=0.00272269786272835 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 2.3949 - val_loss: 1.1985\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8953 - val_loss: 0.6833\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6702 - val_loss: 0.6069\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6136 - val_loss: 0.5691\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5860 - val_loss: 0.5459\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5611 - val_loss: 0.5249\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5414 - val_loss: 0.5064\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.4916\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5103 - val_loss: 0.4790\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4995 - val_loss: 0.4697\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4889 - val_loss: 0.4591\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4801 - val_loss: 0.4515\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4726 - val_loss: 0.4455\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4652 - val_loss: 0.4384\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4591 - val_loss: 0.4324\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4540 - val_loss: 0.4282\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4485 - val_loss: 0.4237\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4450 - val_loss: 0.4202\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4415 - val_loss: 0.4171\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4132\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4355 - val_loss: 0.4113\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4322 - val_loss: 0.4086\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4299 - val_loss: 0.4065\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4273 - val_loss: 0.4036\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4252 - val_loss: 0.4017\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4226 - val_loss: 0.3992\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4213 - val_loss: 0.3983\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4188 - val_loss: 0.3957\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4172 - val_loss: 0.3947\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4156 - val_loss: 0.3930\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4139 - val_loss: 0.3910\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4126 - val_loss: 0.3897\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4108 - val_loss: 0.3895\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4099 - val_loss: 0.3872\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4079 - val_loss: 0.3859\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4072 - val_loss: 0.3846\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4056 - val_loss: 0.3843\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4048 - val_loss: 0.3832\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4035 - val_loss: 0.3813\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4028 - val_loss: 0.3807\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4010 - val_loss: 0.3797\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 0.3788\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3994 - val_loss: 0.3780\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3983 - val_loss: 0.3772\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.3764\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3962 - val_loss: 0.3745\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3953 - val_loss: 0.3749\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3945 - val_loss: 0.3729\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3935 - val_loss: 0.3722\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.3713\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3914 - val_loss: 0.3701\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3904 - val_loss: 0.3694\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3896 - val_loss: 0.3687\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3889 - val_loss: 0.3677\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3881 - val_loss: 0.3671\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3870 - val_loss: 0.3687\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3873 - val_loss: 0.3661\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3859 - val_loss: 0.3652\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3852 - val_loss: 0.3648\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3850 - val_loss: 0.3640\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3840 - val_loss: 0.3650\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.3632\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3825 - val_loss: 0.3661\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3830 - val_loss: 0.3629\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3825 - val_loss: 0.3624\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3620\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3810 - val_loss: 0.3629\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3803 - val_loss: 0.3631\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3805 - val_loss: 0.3611\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3805 - val_loss: 0.3604\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3796 - val_loss: 0.3608\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3794 - val_loss: 0.3601\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3791 - val_loss: 0.3608\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3789 - val_loss: 0.3593\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3781 - val_loss: 0.3594\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3776 - val_loss: 0.3590\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3778 - val_loss: 0.3589\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3772 - val_loss: 0.3583\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3771 - val_loss: 0.3585\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3768 - val_loss: 0.3577\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3764 - val_loss: 0.3576\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3757 - val_loss: 0.3574\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 0.3571\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 0.3565\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3753 - val_loss: 0.3572\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3746 - val_loss: 0.3565\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3744 - val_loss: 0.3566\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3740 - val_loss: 0.3564\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3738 - val_loss: 0.3565\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3734 - val_loss: 0.3555\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3730 - val_loss: 0.3554\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3727 - val_loss: 0.3558\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3727 - val_loss: 0.3556\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 0.3550\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3552\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3718 - val_loss: 0.3547\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3545\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3713 - val_loss: 0.3542\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3708 - val_loss: 0.3540\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3704 - val_loss: 0.3538\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3766\n",
            "[CV]  n_neurons=19, n_hidden=1, learning_rate=0.00272269786272835, total=  48.7s\n",
            "[CV] n_neurons=19, n_hidden=1, learning_rate=0.00272269786272835 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.8855 - val_loss: 0.8276\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7724 - val_loss: 0.6856\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6682 - val_loss: 0.6139\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6128 - val_loss: 0.5694\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5754 - val_loss: 0.5371\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.5115\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.4959\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5093 - val_loss: 0.4825\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4946 - val_loss: 0.4667\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4840 - val_loss: 0.4547\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4718 - val_loss: 0.4441\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4645 - val_loss: 0.4420\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4567 - val_loss: 0.4307\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4512 - val_loss: 0.4240\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4456 - val_loss: 0.4194\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4400 - val_loss: 0.4150\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 0.4112\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4282 - val_loss: 0.4079\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4248 - val_loss: 0.4026\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4209 - val_loss: 0.4025\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4175 - val_loss: 0.3981\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4139 - val_loss: 0.3938\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4112 - val_loss: 0.3918\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4078 - val_loss: 0.3898\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4056 - val_loss: 0.3876\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4032 - val_loss: 0.3859\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4011 - val_loss: 0.3835\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.3815\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3974 - val_loss: 0.3811\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 0.3795\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3944 - val_loss: 0.3771\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3929 - val_loss: 0.3760\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3914 - val_loss: 0.3747\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3901 - val_loss: 0.3735\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3895 - val_loss: 0.3728\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3881 - val_loss: 0.3718\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3871 - val_loss: 0.3709\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3864 - val_loss: 0.3708\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3856 - val_loss: 0.3695\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3850 - val_loss: 0.3696\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.3681\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3828 - val_loss: 0.3669\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.3675\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3810 - val_loss: 0.3723\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3989 - val_loss: 0.3658\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3835 - val_loss: 0.3656\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3800 - val_loss: 0.3656\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3791 - val_loss: 0.3634\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3780 - val_loss: 0.3679\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3770 - val_loss: 0.3624\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3771 - val_loss: 0.3625\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3761 - val_loss: 0.3608\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3607\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3748 - val_loss: 0.3599\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3737 - val_loss: 0.3593\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3728 - val_loss: 0.3591\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3732 - val_loss: 0.3606\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3726 - val_loss: 0.3588\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3581\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3707 - val_loss: 0.3568\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3700 - val_loss: 0.3568\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3700 - val_loss: 0.3564\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3689 - val_loss: 0.3565\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3555\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3681 - val_loss: 0.3553\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.3547\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3675 - val_loss: 0.3552\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.3543\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3663 - val_loss: 0.3546\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3668 - val_loss: 0.3532\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3531\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3655 - val_loss: 0.3536\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3648 - val_loss: 0.3529\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3528\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3646 - val_loss: 0.3527\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3637 - val_loss: 0.3517\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3656 - val_loss: 0.3535\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3632 - val_loss: 0.3513\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3626 - val_loss: 0.3514\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.3521\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3619 - val_loss: 0.3507\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3633 - val_loss: 0.3508\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3619 - val_loss: 0.3501\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3646 - val_loss: 0.3499\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3609 - val_loss: 0.3501\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3612 - val_loss: 0.3493\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3605 - val_loss: 0.3492\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3599 - val_loss: 0.3505\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3600 - val_loss: 0.3488\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.3486\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3594 - val_loss: 0.3482\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.3480\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3587 - val_loss: 0.3483\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3587 - val_loss: 0.3472\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3581 - val_loss: 0.3478\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3591 - val_loss: 0.3471\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3580 - val_loss: 0.3473\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3597 - val_loss: 0.3486\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3616 - val_loss: 0.3466\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3574 - val_loss: 0.3461\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3760\n",
            "[CV]  n_neurons=19, n_hidden=1, learning_rate=0.00272269786272835, total= 1.4min\n",
            "[CV] n_neurons=85, n_hidden=0, learning_rate=0.0006560034570520671 ...\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 5.2407 - val_loss: 3.2852\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.5702 - val_loss: 1.9158\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.5383 - val_loss: 1.2696\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0659 - val_loss: 0.9401\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8289 - val_loss: 0.7661\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7050 - val_loss: 0.6718\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6390 - val_loss: 0.6203\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6032 - val_loss: 0.5915\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5746\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5718 - val_loss: 0.5644\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5649 - val_loss: 0.5579\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5603 - val_loss: 0.5534\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5572 - val_loss: 0.5502\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5547 - val_loss: 0.5474\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5528 - val_loss: 0.5452\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5512 - val_loss: 0.5433\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5496 - val_loss: 0.5416\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5484 - val_loss: 0.5404\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5471 - val_loss: 0.5391\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5458 - val_loss: 0.5376\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 0.5361\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5436 - val_loss: 0.5349\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 0.5337\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5417 - val_loss: 0.5324\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5408 - val_loss: 0.5314\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 0.5307\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5392 - val_loss: 0.5298\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5386 - val_loss: 0.5294\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5378 - val_loss: 0.5285\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 0.5273\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 0.5270\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 0.5260\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5354 - val_loss: 0.5254\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5250\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5344 - val_loss: 0.5242\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5339 - val_loss: 0.5238\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 0.5231\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.5222\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 0.5215\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5213\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 0.5209\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5316 - val_loss: 0.5207\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5202\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5196\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5193\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5195\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.5187\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5299 - val_loss: 0.5184\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5182\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.5177\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 0.5174\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5176\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5169\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5169\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5166\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.5164\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5156\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5157\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5160\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5156\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 0.5151\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 0.5150\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 0.5148\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 0.5148\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 0.5143\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5138\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 0.5140\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5144\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.5141\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 0.5138\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 0.5143\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 0.5133\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.5135\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5134\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 0.5136\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 0.5136\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5131\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5126\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5124\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5132\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5129\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5131\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 0.5123\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5128\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5122\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5125\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5127\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5122\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5261 - val_loss: 0.5128\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 0.5130\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 0.5131\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5122\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 0.5124\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 0.5124\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5118\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.5114\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5111\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5112\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5116\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5115\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5378\n",
            "[CV]  n_neurons=85, n_hidden=0, learning_rate=0.0006560034570520671, total=  43.9s\n",
            "[CV] n_neurons=85, n_hidden=0, learning_rate=0.0006560034570520671 ...\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 5.6810 - val_loss: 3.7160\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.0170 - val_loss: 2.1779\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.8068 - val_loss: 1.4132\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.2187 - val_loss: 1.0176\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.9195 - val_loss: 0.8078\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7633 - val_loss: 0.6948\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6796 - val_loss: 0.6328\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6338 - val_loss: 0.5979\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6079 - val_loss: 0.5775\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5931 - val_loss: 0.5656\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5839 - val_loss: 0.5577\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5781 - val_loss: 0.5524\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 0.5485\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5710 - val_loss: 0.5456\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5685 - val_loss: 0.5433\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5663 - val_loss: 0.5410\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5644 - val_loss: 0.5391\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5626 - val_loss: 0.5374\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5610 - val_loss: 0.5356\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5594 - val_loss: 0.5338\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5580 - val_loss: 0.5322\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5566 - val_loss: 0.5307\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5554 - val_loss: 0.5294\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5541 - val_loss: 0.5282\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5529 - val_loss: 0.5269\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5254\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5508 - val_loss: 0.5245\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5497 - val_loss: 0.5235\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5486 - val_loss: 0.5223\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.5213\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5469 - val_loss: 0.5205\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5459 - val_loss: 0.5194\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5451 - val_loss: 0.5184\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5444 - val_loss: 0.5176\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5437 - val_loss: 0.5168\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 0.5160\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5423 - val_loss: 0.5153\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5418 - val_loss: 0.5147\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.5139\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5405 - val_loss: 0.5135\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5399 - val_loss: 0.5129\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5394 - val_loss: 0.5122\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5389 - val_loss: 0.5116\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5385 - val_loss: 0.5112\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5380 - val_loss: 0.5106\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5101\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5370 - val_loss: 0.5094\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5090\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5364 - val_loss: 0.5086\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 0.5083\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5356 - val_loss: 0.5077\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5353 - val_loss: 0.5074\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5351 - val_loss: 0.5070\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5066\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5345 - val_loss: 0.5065\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5341 - val_loss: 0.5060\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5340 - val_loss: 0.5057\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5337 - val_loss: 0.5055\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5053\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 0.5050\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5330 - val_loss: 0.5046\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5328 - val_loss: 0.5042\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5327 - val_loss: 0.5042\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 0.5039\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5035\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5035\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 0.5031\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5029\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 0.5028\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 0.5025\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.5022\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.5021\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5020\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5017\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5017\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.5018\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 0.5014\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5011\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5010\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5008\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5008\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5006\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5004\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5004\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5004\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.5003\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.5004\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.5004\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5299 - val_loss: 0.5004\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.5002\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.5002\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5001\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.5000\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.4997\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.4997\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.4996\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.4996\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 0.4996\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 0.4996\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.4995\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5405\n",
            "[CV]  n_neurons=85, n_hidden=0, learning_rate=0.0006560034570520671, total= 1.4min\n",
            "[CV] n_neurons=85, n_hidden=0, learning_rate=0.0006560034570520671 ...\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 6.4211 - val_loss: 4.5578\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 3.3927 - val_loss: 2.5610\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.0131 - val_loss: 1.6159\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.3429 - val_loss: 1.1399\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 1.0006 - val_loss: 0.8915\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.8197 - val_loss: 0.7571\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.7209 - val_loss: 0.6824\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6651 - val_loss: 0.6383\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6320 - val_loss: 0.6115\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6115 - val_loss: 0.5941\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5981 - val_loss: 0.5829\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5889 - val_loss: 0.5743\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5679\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 0.5626\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5728 - val_loss: 0.5587\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5693 - val_loss: 0.5548\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5663 - val_loss: 0.5518\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5636 - val_loss: 0.5492\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5612 - val_loss: 0.5461\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5592 - val_loss: 0.5439\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 0.5415\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5555 - val_loss: 0.5402\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5540 - val_loss: 0.5384\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5524 - val_loss: 0.5363\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5512 - val_loss: 0.5346\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5499 - val_loss: 0.5330\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5487 - val_loss: 0.5315\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5477 - val_loss: 0.5302\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5467 - val_loss: 0.5290\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5456 - val_loss: 0.5275\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5448 - val_loss: 0.5267\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5439 - val_loss: 0.5258\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 0.5245\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5423 - val_loss: 0.5242\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5416 - val_loss: 0.5230\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5409 - val_loss: 0.5221\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5401 - val_loss: 0.5208\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5396 - val_loss: 0.5202\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5390 - val_loss: 0.5197\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5384 - val_loss: 0.5195\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5377 - val_loss: 0.5181\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5373 - val_loss: 0.5172\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5175\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 0.5162\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5357 - val_loss: 0.5150\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 0.5152\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5146\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5345 - val_loss: 0.5137\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5136\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5338 - val_loss: 0.5140\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 0.5132\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.5129\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 0.5118\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 0.5115\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 0.5120\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5115\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 0.5114\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5103\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5102\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5092\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5086\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5090\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5299 - val_loss: 0.5083\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5298 - val_loss: 0.5080\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5296 - val_loss: 0.5088\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5292 - val_loss: 0.5077\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5070\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 0.5070\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5075\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5285 - val_loss: 0.5075\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5068\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5068\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5066\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5279 - val_loss: 0.5061\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5061\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 0.5059\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 0.5061\n",
            "Epoch 78/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 0.5067\n",
            "Epoch 79/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 0.5060\n",
            "Epoch 80/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 0.5049\n",
            "Epoch 81/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.5048\n",
            "Epoch 82/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.5044\n",
            "Epoch 83/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5041\n",
            "Epoch 84/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5044\n",
            "Epoch 85/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 0.5051\n",
            "Epoch 86/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5044\n",
            "Epoch 87/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5050\n",
            "Epoch 88/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5057\n",
            "Epoch 89/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5261 - val_loss: 0.5051\n",
            "Epoch 90/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5041\n",
            "Epoch 91/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.5035\n",
            "Epoch 92/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.5046\n",
            "Epoch 93/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5036\n",
            "Epoch 94/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5256 - val_loss: 0.5031\n",
            "Epoch 95/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5038\n",
            "Epoch 96/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 0.5032\n",
            "Epoch 97/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.5025\n",
            "Epoch 98/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5255 - val_loss: 0.5026\n",
            "Epoch 99/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.5029\n",
            "Epoch 100/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.5032\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5687\n",
            "[CV]  n_neurons=85, n_hidden=0, learning_rate=0.0006560034570520671, total= 1.4min\n",
            "[CV] n_neurons=17, n_hidden=0, learning_rate=0.011966801511868859 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 2.3038 - val_loss: 1.9061\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 24.9585 - val_loss: 22.2175\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 566.1917 - val_loss: 518.5609\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 4924.6045 - val_loss: 12045.2295\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 138822.1875 - val_loss: 256886.8281\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 6408621.5000 - val_loss: 5599928.0000\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 65656948.0000 - val_loss: 126604200.0000\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2331019008.0000 - val_loss: 2818763776.0000\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 49182957568.0000 - val_loss: 61958574080.0000\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 741063196672.0000 - val_loss: 1383941931008.0000\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 12379438972928.0000 - val_loss: 33747091062784.0000\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 183253084405760.0000\n",
            "[CV]  n_neurons=17, n_hidden=0, learning_rate=0.011966801511868859, total=   5.8s\n",
            "[CV] n_neurons=17, n_hidden=0, learning_rate=0.011966801511868859 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3686 - val_loss: 1.2582\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 2.6331 - val_loss: 2.8955\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 10.5432 - val_loss: 9.8724\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 625.6976 - val_loss: 49.2625\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 164.4504 - val_loss: 143.9097\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 588.5730 - val_loss: 535.7112\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 43477.5625 - val_loss: 2148.6260\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 9810.2793 - val_loss: 8424.1914\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 34665.7148 - val_loss: 31505.4102\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 191753.6250 - val_loss: 118604.4688\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 450040.2500 - val_loss: 473713.6562\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 138893.3125\n",
            "[CV]  n_neurons=17, n_hidden=0, learning_rate=0.011966801511868859, total=   5.7s\n",
            "[CV] n_neurons=17, n_hidden=0, learning_rate=0.011966801511868859 ....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.0629 - val_loss: 0.5320\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5655 - val_loss: 0.5380\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5490 - val_loss: 0.5116\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5535 - val_loss: 0.5021\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 0.4986\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.5151\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 0.5129\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5659 - val_loss: 0.5090\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5392 - val_loss: 0.4997\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5771 - val_loss: 0.5015\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5345 - val_loss: 0.5291\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5186\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5463 - val_loss: 0.4992\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5508 - val_loss: 0.5212\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.5532 - val_loss: 0.5083\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.5323\n",
            "[CV]  n_neurons=17, n_hidden=0, learning_rate=0.011966801511868859, total=   7.4s\n",
            "[CV] n_neurons=3, n_hidden=2, learning_rate=0.014074860627621732 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 1.3029 - val_loss: 0.7371\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.5718\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5490 - val_loss: 0.5031\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4903 - val_loss: 0.4558\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4621 - val_loss: 0.4350\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4431 - val_loss: 0.4019\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4413 - val_loss: 0.4279\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4269 - val_loss: 0.3923\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4166 - val_loss: 0.4155\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4122 - val_loss: 0.3790\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4029 - val_loss: 0.3775\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4003 - val_loss: 0.3859\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3966 - val_loss: 0.3704\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4014 - val_loss: 0.3710\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4037 - val_loss: 0.3977\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.3794\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3915 - val_loss: 0.3723\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3895 - val_loss: 0.3628\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3872 - val_loss: 0.3670\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3850 - val_loss: 0.3588\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3853 - val_loss: 0.3587\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.3707\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3801 - val_loss: 0.3622\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.3649\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3793 - val_loss: 0.3658\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3796 - val_loss: 0.3581\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3810 - val_loss: 0.3598\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3766 - val_loss: 0.3598\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3801 - val_loss: 0.3622\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3784 - val_loss: 0.3573\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4113 - val_loss: 0.3863\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3869 - val_loss: 0.3684\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3783 - val_loss: 0.3617\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3768 - val_loss: 0.3554\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3770 - val_loss: 0.3560\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4035 - val_loss: 0.4057\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.3751\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3801 - val_loss: 0.3524\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3765 - val_loss: 0.3532\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3734 - val_loss: 0.3554\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3736 - val_loss: 0.3567\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3737 - val_loss: 0.3579\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3752 - val_loss: 0.3471\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.3509\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3758 - val_loss: 0.3558\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 0.3547\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3714 - val_loss: 0.3519\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3734 - val_loss: 0.3606\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3747 - val_loss: 0.3470\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3733 - val_loss: 0.3496\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3733 - val_loss: 0.3550\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3696 - val_loss: 0.3535\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3702 - val_loss: 0.3539\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3863 - val_loss: 0.3751\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3779 - val_loss: 0.3546\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3714 - val_loss: 0.3530\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3716 - val_loss: 0.3490\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3693 - val_loss: 0.3475\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3728 - val_loss: 0.3513\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.3791\n",
            "[CV]  n_neurons=3, n_hidden=2, learning_rate=0.014074860627621732, total=  30.8s\n",
            "[CV] n_neurons=3, n_hidden=2, learning_rate=0.014074860627621732 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.5935 - val_loss: 1.2272\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.9960 - val_loss: 0.6482\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.6186 - val_loss: 0.5433\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5388 - val_loss: 0.4777\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4812 - val_loss: 0.4328\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4492 - val_loss: 0.4108\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4335 - val_loss: 0.4045\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4260 - val_loss: 0.4047\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4244 - val_loss: 0.3936\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4212 - val_loss: 0.3953\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4197 - val_loss: 0.3966\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4166 - val_loss: 0.3983\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4142 - val_loss: 0.3982\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4142 - val_loss: 0.3948\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4120 - val_loss: 0.3885\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4114 - val_loss: 0.3858\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4112 - val_loss: 0.3883\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4088 - val_loss: 0.3967\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4087 - val_loss: 0.3999\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4095 - val_loss: 0.3902\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4076 - val_loss: 0.3900\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4070 - val_loss: 0.3821\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4062 - val_loss: 0.3833\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4058 - val_loss: 0.3819\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4052 - val_loss: 0.3784\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4039 - val_loss: 0.3851\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4039 - val_loss: 0.3815\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4032 - val_loss: 0.3864\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4030 - val_loss: 0.3874\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4028 - val_loss: 0.3838\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4032 - val_loss: 0.3812\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4033 - val_loss: 0.3858\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4031 - val_loss: 0.3859\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4026 - val_loss: 0.3845\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4033 - val_loss: 0.3850\n",
            "121/121 [==============================] - 0s 2ms/step - loss: 0.4051\n",
            "[CV]  n_neurons=3, n_hidden=2, learning_rate=0.014074860627621732, total=  19.0s\n",
            "[CV] n_neurons=3, n_hidden=2, learning_rate=0.014074860627621732 .....\n",
            "Epoch 1/100\n",
            "242/242 [==============================] - 1s 3ms/step - loss: 1.3344 - val_loss: 0.6391\n",
            "Epoch 2/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5900 - val_loss: 0.5309\n",
            "Epoch 3/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.5068 - val_loss: 0.4649\n",
            "Epoch 4/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4589 - val_loss: 0.4345\n",
            "Epoch 5/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4367 - val_loss: 0.4139\n",
            "Epoch 6/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4233 - val_loss: 0.4066\n",
            "Epoch 7/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4143 - val_loss: 0.3973\n",
            "Epoch 8/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4100 - val_loss: 0.3951\n",
            "Epoch 9/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4082 - val_loss: 0.3927\n",
            "Epoch 10/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4080 - val_loss: 0.4016\n",
            "Epoch 11/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4052 - val_loss: 0.3918\n",
            "Epoch 12/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4059 - val_loss: 0.3935\n",
            "Epoch 13/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4043 - val_loss: 0.3900\n",
            "Epoch 14/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4031 - val_loss: 0.4033\n",
            "Epoch 15/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4052 - val_loss: 0.3925\n",
            "Epoch 16/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4041 - val_loss: 0.3915\n",
            "Epoch 17/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4026 - val_loss: 0.3953\n",
            "Epoch 18/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4024 - val_loss: 0.3893\n",
            "Epoch 19/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4021 - val_loss: 0.3877\n",
            "Epoch 20/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4025 - val_loss: 0.3843\n",
            "Epoch 21/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4004 - val_loss: 0.3939\n",
            "Epoch 22/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.3817\n",
            "Epoch 23/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4006 - val_loss: 0.3927\n",
            "Epoch 24/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.4015 - val_loss: 0.3873\n",
            "Epoch 25/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4011 - val_loss: 0.3845\n",
            "Epoch 26/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.4002 - val_loss: 0.3838\n",
            "Epoch 27/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3995 - val_loss: 0.3818\n",
            "Epoch 28/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3993 - val_loss: 0.3813\n",
            "Epoch 29/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3995 - val_loss: 0.3841\n",
            "Epoch 30/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3999 - val_loss: 0.3836\n",
            "Epoch 31/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3985 - val_loss: 0.3813\n",
            "Epoch 32/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3994 - val_loss: 0.3810\n",
            "Epoch 33/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3995 - val_loss: 0.3985\n",
            "Epoch 34/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3802\n",
            "Epoch 35/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.3822\n",
            "Epoch 36/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.3881\n",
            "Epoch 37/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3988 - val_loss: 0.3857\n",
            "Epoch 38/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3980 - val_loss: 0.3836\n",
            "Epoch 39/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.3794\n",
            "Epoch 40/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3987 - val_loss: 0.3838\n",
            "Epoch 41/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3962 - val_loss: 0.3856\n",
            "Epoch 42/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.3812\n",
            "Epoch 43/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3973 - val_loss: 0.3811\n",
            "Epoch 44/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.3839\n",
            "Epoch 45/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3979 - val_loss: 0.3888\n",
            "Epoch 46/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3980 - val_loss: 0.3808\n",
            "Epoch 47/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.3801\n",
            "Epoch 48/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3973 - val_loss: 0.3841\n",
            "Epoch 49/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3969 - val_loss: 0.3787\n",
            "Epoch 50/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3959 - val_loss: 0.3812\n",
            "Epoch 51/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3965 - val_loss: 0.3785\n",
            "Epoch 52/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.3830\n",
            "Epoch 53/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3967 - val_loss: 0.3770\n",
            "Epoch 54/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3957 - val_loss: 0.3764\n",
            "Epoch 55/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3955 - val_loss: 0.3781\n",
            "Epoch 56/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3956 - val_loss: 0.3810\n",
            "Epoch 57/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3971 - val_loss: 0.3787\n",
            "Epoch 58/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3803\n",
            "Epoch 59/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.3763\n",
            "Epoch 60/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3968 - val_loss: 0.3796\n",
            "Epoch 61/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.3783\n",
            "Epoch 62/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3967 - val_loss: 0.3839\n",
            "Epoch 63/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3953 - val_loss: 0.3798\n",
            "Epoch 64/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3958 - val_loss: 0.3803\n",
            "Epoch 65/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3959 - val_loss: 0.3765\n",
            "Epoch 66/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3791\n",
            "Epoch 67/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3954 - val_loss: 0.3758\n",
            "Epoch 68/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.3918\n",
            "Epoch 69/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3961 - val_loss: 0.3803\n",
            "Epoch 70/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.3773\n",
            "Epoch 71/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3950 - val_loss: 0.3761\n",
            "Epoch 72/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3963 - val_loss: 0.3810\n",
            "Epoch 73/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3952 - val_loss: 0.3798\n",
            "Epoch 74/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3948 - val_loss: 0.3777\n",
            "Epoch 75/100\n",
            "242/242 [==============================] - 1s 2ms/step - loss: 0.3949 - val_loss: 0.3785\n",
            "Epoch 76/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3952 - val_loss: 0.3789\n",
            "Epoch 77/100\n",
            "242/242 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.3769\n",
            "121/121 [==============================] - 0s 1ms/step - loss: 0.4019\n",
            "[CV]  n_neurons=3, n_hidden=2, learning_rate=0.014074860627621732, total=  41.6s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 17.2min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6611 - val_loss: 0.4079\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4391 - val_loss: 0.3696\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3858 - val_loss: 0.3589\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3656 - val_loss: 0.3387\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3520 - val_loss: 0.3505\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3461 - val_loss: 0.3400\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3392 - val_loss: 0.3201\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3320 - val_loss: 0.3185\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3272 - val_loss: 0.3124\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3215 - val_loss: 0.3536\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3289 - val_loss: 0.3186\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3131 - val_loss: 0.2958\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3117 - val_loss: 0.2998\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3061 - val_loss: 0.3162\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3007 - val_loss: 0.2972\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2998 - val_loss: 0.3003\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2954 - val_loss: 0.2903\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3023 - val_loss: 0.2901\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2907 - val_loss: 0.2809\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2911 - val_loss: 0.2763\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2869 - val_loss: 0.2899\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2834 - val_loss: 0.2794\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2812 - val_loss: 0.2952\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2799 - val_loss: 0.2757\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2793 - val_loss: 0.2791\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2779 - val_loss: 0.3067\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2757 - val_loss: 0.2675\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2735 - val_loss: 0.2804\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2716 - val_loss: 0.2718\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2707 - val_loss: 0.2697\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2689 - val_loss: 0.2936\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2684 - val_loss: 0.3003\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2659 - val_loss: 0.2672\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2652 - val_loss: 0.2700\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2640 - val_loss: 0.2632\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2744 - val_loss: 0.2885\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2613 - val_loss: 0.2634\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2616 - val_loss: 0.2622\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2593 - val_loss: 0.2885\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2605 - val_loss: 0.2610\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2566 - val_loss: 0.2654\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2564 - val_loss: 0.2673\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2568 - val_loss: 0.2773\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2560 - val_loss: 0.2679\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2545 - val_loss: 0.2579\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2550 - val_loss: 0.2758\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2518 - val_loss: 0.2716\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2566 - val_loss: 0.2691\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2505 - val_loss: 0.2588\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2497 - val_loss: 0.2583\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2496 - val_loss: 0.2617\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2499 - val_loss: 0.2663\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2488 - val_loss: 0.2905\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2494 - val_loss: 0.2674\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.2492 - val_loss: 0.2791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7f212b0d8a10>,\n",
              "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
              "                   param_distributions={'learning_rate': [0.002061384396882985,\n",
              "                                                          0.000496919217408326,\n",
              "                                                          0.00790255882354201,\n",
              "                                                          0.00046588444929100903,\n",
              "                                                          0.0074825709234574945,\n",
              "                                                          0.00033683792825179107,\n",
              "                                                          0.00031907107302413354,\n",
              "                                                          0.0...\n",
              "                                                          0.019450550864431003,\n",
              "                                                          0.0040700046477898585,\n",
              "                                                          0.011834228628824705,\n",
              "                                                          0.0008823037712403099,\n",
              "                                                          0.0007033042820463598,\n",
              "                                                          0.000940901572595106, ...],\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
              "                                                      10, 11, 12, 13, 14, 15,\n",
              "                                                      16, 17, 18, 19, 20, 21,\n",
              "                                                      22, 23, 24, 25, 26, 27,\n",
              "                                                      28, 29, 30, ...]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC-wtoyUsU1Z",
        "outputId": "ab12380a-ea93-4781-dec9-1d9d60d15ea0"
      },
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.012281399782266571, 'n_hidden': 3, 'n_neurons': 91}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7j08vUTsVHG",
        "outputId": "d1d85416-4ff2-4780-e02d-298891ce6369"
      },
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.29035115242004395"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpI91EDsd27"
      },
      "source": [
        "model = rnd_search_cv.best_estimator_.model"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcVo2cyfsqxD"
      },
      "source": [
        "**랜덤서치의 단점**\n",
        "\n",
        "- 하이퍼파라미터 공간이 크면 시간이 오래 걸린다..\n",
        "- 하이퍼파라미터의 값의 범위를 크게해서 일단 첫 범위를 찾고, 그 중에 좋은 곳에서 또 다시 랜덤서치함.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9KQsGCQs_FC"
      },
      "source": [
        "**더 효율적인 탐색 기법**\n",
        "\n",
        "- 탐색지역이 좋다고 판명될 때 더 탐색을 수행한다!!\n",
        "- 이용할 수 있는 라이브러리들..\n",
        "    - Hyperpot\n",
        "    - Hyperas, kopt, Talos\n",
        "    - 케라스 튜너\n",
        "    - Scikit-Optimize(skopt)\n",
        "    - Spearmint\n",
        "    - Hyperband\n",
        "    - Sklearn-Deap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpuSG_6fYB-x"
      },
      "source": [
        "##10.3.1 은닉층 개수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw5TVHS3tZUW"
      },
      "source": [
        "**입력층, 출력층의 갯수**\n",
        "\n",
        "- 해당 작업에 필요한 입력과 출력의 형태에 따라 결정됨..\n",
        "- ex) 숫자 mnist : 28*28개의 입력뉴런, 10개의 출력뉴런\n",
        "\n",
        "**은닉층의 구성**\n",
        "\n",
        "- 일반적으로 각 층의 뉴런을 점점 줄여서 깔때기처럼 구성\n",
        "- 첫번째 은닉층을 크게 하는 것이 도움이 됨..\n",
        "- 실전에서는 **필요한 것보다 더 많은 층과 뉴런을 가진 모델을 선택하고, 조기종료나 규제기법을 사용하는 것**이 효과적임..\n",
        "- 한 층의 뉴런 수가 너무 적으면 입력의 유용한 특성을 충분히 표현하지 못한다..\n",
        "    - ex) 3D데이터인데 뉴런이 2개라면 다 표현을 못한다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFQONzbYYB7z"
      },
      "source": [
        "##10.3.2 은닉층의 뉴런 개수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySL8Pts0YBz0"
      },
      "source": [
        "##10.3.3 학습률, 배치 크기 그리고 다른 하이퍼파라미터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOMSF0FXZSSf"
      },
      "source": [
        "###학습률\n",
        "\n",
        "- 매우 낮은 학습률에서 점점 키워가면서 조정..\n",
        "- 다른 하이퍼파라미터 조정 후 다시 탐색해야함.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4VKZ4O5ZSGz"
      },
      "source": [
        "###옵티마이저\n",
        "\n",
        "- 좋은 옵티마이저를 선택해라.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxrHrNPyZR8k"
      },
      "source": [
        "###배치 크기\n",
        "\n",
        "- 크게 좋다는 쪽도 있고\n",
        "- 32개의 배치 사이즈로 하는게 국룰이라는 쪽도 있다.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45R3rwVBZRyk"
      },
      "source": [
        "###활성화 함수\n",
        "\n",
        "- 기본값 : Relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX9pvEghZRoW"
      },
      "source": [
        "###반복 획수\n",
        "\n",
        "- 이건 조정하기 보다는 엄청 크게 한 다음 조기종료하는게 더 이득.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-2k9RVyZRgj"
      },
      "source": [
        "#10.4 연습문제"
      ]
    }
  ]
}